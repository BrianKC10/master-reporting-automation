{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e77cf64-f14c-46fd-bfa8-9420f973d289",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T00:01:42.253173Z",
     "iopub.status.busy": "2025-07-03T00:01:42.253087Z",
     "iopub.status.idle": "2025-07-03T00:01:43.508227Z",
     "shell.execute_reply": "2025-07-03T00:01:43.507964Z",
     "shell.execute_reply.started": "2025-07-03T00:01:42.253164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 1: Imports and Global Settings - Executed\n"
     ]
    }
   ],
   "source": [
    "# Basic Python/Data Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "import re\n",
    "\n",
    "\n",
    "# Add this import at the top of your cell\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# Salesforce Integration\n",
    "from simple_salesforce import Salesforce\n",
    "import requests\n",
    "\n",
    "# Forecasting Library\n",
    "from prophet import Prophet\n",
    "# from prophet.plot import plot_plotly, plot_components_plotly # For interactive plots\n",
    "\n",
    "# Pandas Display Settings\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "pd.set_option('display.max_columns', None) \n",
    "pd.set_option('display.width', 1000) \n",
    "\n",
    "# Matplotlib/Seaborn Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "\n",
    "pd.set_option('display.width', 2000)      # Set a large width to prevent line wrapping\n",
    "pd.set_option('display.max_columns', None)  # Ensure all columns are shown\n",
    "pd.set_option('display.max_rows', 200)      # Optional: show more rows if needed\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format) # Optional: format numbers for readability\n",
    "\n",
    "\n",
    "print(\"Cell 1: Imports and Global Settings - Executed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf6d19b-b0bc-4dcd-aeb9-32f9627fb607",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T00:01:43.508962Z",
     "iopub.status.busy": "2025-07-03T00:01:43.508739Z",
     "iopub.status.idle": "2025-07-03T00:01:43.511889Z",
     "shell.execute_reply": "2025-07-03T00:01:43.511648Z",
     "shell.execute_reply.started": "2025-07-03T00:01:43.508950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 2: Salesforce Configuration - User: bchen@envoy.com, Report: 00OUO000009IZVD2A4, Start FY: 2023 - Set\n"
     ]
    }
   ],
   "source": [
    "# --- General Percentage Change Thresholds ---\n",
    "# A metric will be flagged as a \"Drop/Decrease\" if its value falls by this percentage or more. Use a negative number.\n",
    "PCT_CHANGE_THRESHOLD_DROP = -20\n",
    "\n",
    "# A metric will be flagged as an \"Increase\" if its value rises by this percentage or more. Use a positive number.\n",
    "PCT_CHANGE_THRESHOLD_INCREASE = 20\n",
    "\n",
    "\n",
    "\n",
    "# --- Absolute Change Thresholds (used in combination with percentage changes for some metrics) ---\n",
    "# For 'Total ARR', a change must meet BOTH the percentage threshold AND this absolute dollar amount to be flagged.\n",
    "# This prevents flagging a 50% drop on a very small number (e.g., from $100 to $50) as \"significant\".\n",
    "ABS_ARR_CHANGE_THRESHOLD = 50000\n",
    "\n",
    "# For '# of Opps' and '# of Won Opps', a change must meet BOTH the percentage threshold AND this absolute opportunity count.\n",
    "# This avoids flagging a 100% change from 1 opp to 2 opps as significant.\n",
    "ABS_OPP_CHANGE_THRESHOLD = 5\n",
    "# For 'Avg Sales Price', a change must meet BOTH the percentage threshold AND this absolute dollar amount.\n",
    "ASP_CHANGE_THRESHOLD = 500\n",
    "\n",
    "\n",
    "# --- Specific Metric Thresholds (these work independently) ---\n",
    "# For 'Win Rate (Count)', flags a change if the rate moves by this many absolute percentage points.\n",
    "# Example: If set to 5, a change from 30% to 24% (a -6 point change) would be flagged as a significant drop.\n",
    "# This is often more useful than a relative percentage change for win rates.\n",
    "WIN_RATE_PCT_POINT_CHANGE_THRESHOLD = 5\n",
    "\n",
    "# For 'Avg Sales Cycle', a change is flagged if it meets EITHER the absolute day change OR the percentage change threshold.\n",
    "# This captures both small-but-fast relative changes and large absolute changes.\n",
    "# Flags an increase/decrease if the sales cycle changes by at least this many days.\n",
    "SALES_CYCLE_DAY_CHANGE_THRESHOLD = 10\n",
    "# Flags an increase/decrease if the sales cycle changes by at least this percentage.\n",
    "SALES_CYCLE_PCT_CHANGE_THRESHOLD = 15\n",
    "\n",
    "# --- Stage Name Configuration ---\n",
    "# Define the exact names of your \"Won\" and \"Lost\" stages from Salesforce.\n",
    "# This is critical for calculating Win Rate correctly.\n",
    "STAGE_WON = 'Closed Won'\n",
    "STAGE_LOST = 'Closed Lost'\n",
    "\n",
    "# ## Salesforce Configuration & Credentials\n",
    "\n",
    "# --- USER: Salesforce Credentials & Report ID ---\n",
    "SF_USERNAME = 'bchen@envoy.com' # Replace with your username\n",
    "SF_PASSWORD = 'TasksandEvents1'   # Replace with your password\n",
    "SF_SECURITY_TOKEN = 'nQWlT8vNdnJwxwtfpS1ic4Z7O' # Replace with your security token\n",
    "SF_INSTANCE_URL = 'https://envoy.my.salesforce.com/'\n",
    "MAIN_REPORT_ID = '00OUO000009IZVD2A4' # Replace with your main Salesforce Report ID\n",
    "\n",
    "# --- Other Configurations ---\n",
    "# Fiscal year definition: FY2023 starts on Feb 1, 2022. We will filter for data from FY2023 onwards.\n",
    "START_FISCAL_YEAR_FOR_ANALYSIS = 2023 \n",
    "\n",
    "print(f\"Cell 2: Salesforce Configuration - User: {SF_USERNAME}, Report: {MAIN_REPORT_ID}, Start FY: {START_FISCAL_YEAR_FOR_ANALYSIS} - Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63ae4f8f-db76-4121-9f7e-92aa1a898167",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T00:01:43.512448Z",
     "iopub.status.busy": "2025-07-03T00:01:43.512337Z",
     "iopub.status.idle": "2025-07-03T00:01:43.519777Z",
     "shell.execute_reply": "2025-07-03T00:01:43.519206Z",
     "shell.execute_reply.started": "2025-07-03T00:01:43.512439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 3: Utility Functions - Defined\n"
     ]
    }
   ],
   "source": [
    "# --- Function to Download Salesforce Report ---\n",
    "def download_sf_report(sf_instance, report_id, sf_session):\n",
    "    if sf_session is None:\n",
    "        print(\"Salesforce session not available. Cannot download report.\")\n",
    "        return pd.DataFrame()\n",
    "    export_params = '?isdtp=p1&export=1&enc=UTF-8&xf=csv'\n",
    "    sf_url = f\"{sf_instance}{report_id}{export_params}\"\n",
    "    try:\n",
    "        response = requests.get(sf_url, headers=sf_session.headers, cookies={'sid': sf_session.session_id})\n",
    "        response.raise_for_status()\n",
    "        report_content = response.content.decode('utf-8')\n",
    "        lines = report_content.splitlines()\n",
    "        num_footer_lines = 0\n",
    "        for i in range(len(lines) - 1, max(0, len(lines) - 10), -1): # Check last 10 lines\n",
    "            line_strip = lines[i].strip()\n",
    "            if \"Confidential Information - Do Not Distribute\" in line_strip or \\\n",
    "               \"Report Generation Date\" in line_strip or \\\n",
    "               \"Envoy\" == line_strip or \\\n",
    "               len(line_strip) == 0:\n",
    "                num_footer_lines +=1\n",
    "            else:\n",
    "                break\n",
    "        report_content_cleaned = \"\\n\".join(lines[:-num_footer_lines]) if num_footer_lines > 0 else report_content\n",
    "        df_report = pd.read_csv(StringIO(report_content_cleaned))\n",
    "        print(f\"Report {report_id} downloaded successfully. Shape: {df_report.shape}\")\n",
    "        return df_report\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading report {report_id}: {e}\")\n",
    "        if 'response' in locals() and response is not None: print(f\"Response content (first 500 chars): {response.text[:500]}\")\n",
    "        return pd.DataFrame()\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Report {report_id} is empty or not parsed correctly.\")\n",
    "        if 'report_content' in locals(): print(f\"Raw content (first 500 chars): {report_content[:500]}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing report {report_id}: {e}\")\n",
    "        if 'report_content' in locals(): print(f\"Raw content (first 500 chars): {report_content[:500]}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# --- Fiscal Quarter Calculation Function ---\n",
    "def get_fiscal_quarter_info(date_input):\n",
    "    if pd.isnull(date_input): return pd.NaT, None, None\n",
    "    \n",
    "    date_to_process = date_input\n",
    "    if isinstance(date_to_process, str): date_to_process = pd.to_datetime(date_to_process, errors='coerce')\n",
    "    elif not isinstance(date_to_process, pd.Timestamp):\n",
    "        try: date_to_process = pd.to_datetime(date_to_process, errors='coerce')\n",
    "        except Exception: return pd.NaT, None, None\n",
    "    \n",
    "    if pd.NaT is date_to_process : return pd.NaT, None, None \n",
    "\n",
    "    year, month = date_to_process.year, date_to_process.month\n",
    "    fiscal_year_label_num, quarter_start_date, quarter_label_str = None, pd.NaT, None\n",
    "\n",
    "    if month == 1:\n",
    "        quarter_start_date, fiscal_year_label_num, quarter_label_str = pd.Timestamp(year=year-1, month=11, day=1), year, f'FY{year}Q4'\n",
    "    elif month in [2, 3, 4]:\n",
    "        quarter_start_date, fiscal_year_label_num, quarter_label_str = pd.Timestamp(year=year, month=2, day=1), year + 1, f'FY{year+1}Q1'\n",
    "    elif month in [5, 6, 7]:\n",
    "        quarter_start_date, fiscal_year_label_num, quarter_label_str = pd.Timestamp(year=year, month=5, day=1), year + 1, f'FY{year+1}Q2'\n",
    "    elif month in [8, 9, 10]:\n",
    "        quarter_start_date, fiscal_year_label_num, quarter_label_str = pd.Timestamp(year=year, month=8, day=1), year + 1, f'FY{year+1}Q3'\n",
    "    elif month in [11, 12]:\n",
    "        quarter_start_date, fiscal_year_label_num, quarter_label_str = pd.Timestamp(year=year, month=11, day=1), year + 1, f'FY{year+1}Q4'\n",
    "    \n",
    "    return quarter_start_date, quarter_label_str, fiscal_year_label_num\n",
    "\n",
    "print(\"Cell 3: Utility Functions - Defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c6c469-5a95-4e4e-aa11-904de331c907",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T00:01:43.520602Z",
     "iopub.status.busy": "2025-07-03T00:01:43.520477Z",
     "iopub.status.idle": "2025-07-03T00:01:47.920087Z",
     "shell.execute_reply": "2025-07-03T00:01:47.919810Z",
     "shell.execute_reply.started": "2025-07-03T00:01:43.520587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salesforce authentication successful.\n",
      "Report 00OUO000009IZVD2A4 downloaded successfully. Shape: (14718, 35)\n",
      "Raw DataFrame loaded with shape: (14718, 35)\n"
     ]
    }
   ],
   "source": [
    "# Authenticate to Salesforce\n",
    "sf_session = None # Initialize\n",
    "try:\n",
    "    sf_session = Salesforce(username=SF_USERNAME, password=SF_PASSWORD, security_token=SF_SECURITY_TOKEN)\n",
    "    print(\"Salesforce authentication successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Salesforce authentication failed: {e}\")\n",
    "    sf_session = None # Ensure it's None on failure\n",
    "\n",
    "# Download the main dataframe\n",
    "raw_df = pd.DataFrame() # Initialize\n",
    "if sf_session:\n",
    "    raw_df = download_sf_report(SF_INSTANCE_URL, MAIN_REPORT_ID, sf_session)\n",
    "else:\n",
    "    print(\"Skipping report download due to authentication failure.\")\n",
    "\n",
    "if raw_df.empty:\n",
    "    print(\"WARNING: Raw DataFrame is empty. Subsequent cells may fail or produce no results.\")\n",
    "else:\n",
    "    print(f\"Raw DataFrame loaded with shape: {raw_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "xu2e1br0zv8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T00:01:47.920709Z",
     "iopub.status.busy": "2025-07-03T00:01:47.920627Z",
     "iopub.status.idle": "2025-07-03T00:01:48.057266Z",
     "shell.execute_reply": "2025-07-03T00:01:48.057027Z",
     "shell.execute_reply.started": "2025-07-03T00:01:47.920700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: 'ARR Change' column processed as the primary value metric.\n",
      "Info: 'Inquarter Booking Flag' calculated on the full dataset.\n",
      "Segment filtering: Excluded 148 records from segments: ['Self Serve', 'Self Service', 'Unknown']\n",
      "Remaining records after segment filtering: 14570\n",
      "\n",
      "Preprocessing complete on filtered dataset. Shape: (14570, 36)\n",
      "Opportunity ID Column used: SFDC ID 18 Digit\n",
      "In-Quarter DataFrame (inq_df) shape: (5971, 36)\n",
      "Cell 5: Data Preprocessing with Segment Filtering - Completed Successfully\n"
     ]
    }
   ],
   "source": [
    "# ## Data Preprocessing\n",
    "#\n",
    "# ### Key Steps:\n",
    "# 1.  **Date Conversion:** Convert all relevant date columns to a proper datetime format.\n",
    "# 2.  **ARR Processing:** Use `'ARR Change'` as the primary value metric. If it's not available, fall back to `'ARR'` or create a zero-value column to prevent errors. This ensures the notebook is robust.\n",
    "# 3.  **Fiscal Period Calculation:** For each opportunity, calculate its fiscal quarter based on both its `Created Date` and `Close Date` using the `get_fiscal_quarter_info` utility.\n",
    "# 4.  **In-Quarter Flag:** Create a boolean flag, `'Inquarter Booking Flag'`, which is `True` if an opportunity's creation and close fiscal quarters are the same.\n",
    "# 5.  **Opportunity ID:** Dynamically find the correct Opportunity ID column from a list of potential names to make the notebook more adaptable to different report formats.\n",
    "# 6.  **In-Quarter DataFrame (`inq_df`):** Create a separate DataFrame containing only the deals flagged as in-quarter bookings for focused analysis and forecasting.\n",
    "# 7.  **Segment Filtering:** Filter out Self Serve, Self Service, and Unknown segments from analysis.\n",
    "\n",
    "processed_df = pd.DataFrame()\n",
    "inq_df = pd.DataFrame()\n",
    "OPP_ID_COL_NAME = None \n",
    "\n",
    "if not raw_df.empty:\n",
    "    processed_df = raw_df.copy()\n",
    "\n",
    "    # 1. Convert Date Columns\n",
    "    date_cols_to_convert = ['Created Date', 'Close Date', 'Renewal Date', 'Opportunity Created Date']\n",
    "    for col in date_cols_to_convert:\n",
    "        if col in processed_df.columns:\n",
    "            processed_df[col] = pd.to_datetime(processed_df[col], errors='coerce')\n",
    "\n",
    "    # 2. Process ARR Column (Using 'ARR Change')\n",
    "    if 'ARR Change' in processed_df.columns:\n",
    "        processed_df['ARR Change'] = pd.to_numeric(processed_df['ARR Change'], errors='coerce').fillna(0)\n",
    "        print(\"Info: 'ARR Change' column processed as the primary value metric.\")\n",
    "    else:\n",
    "        # If 'ARR Change' is missing, fall back to 'ARR' or create a zero column.\n",
    "        if 'ARR' in processed_df.columns:\n",
    "            print(\"Warning: 'ARR Change' column not found. Falling back to using 'ARR'.\")\n",
    "            processed_df.rename(columns={'ARR': 'ARR Change'}, inplace=True) # Rename for consistency downstream\n",
    "            processed_df['ARR Change'] = pd.to_numeric(processed_df['ARR Change'], errors='coerce').fillna(0)\n",
    "        else:\n",
    "            print(\"CRITICAL WARNING: Neither 'ARR Change' nor 'ARR' column found. Creating 'ARR Change' with zeros.\")\n",
    "            processed_df['ARR Change'] = 0\n",
    "\n",
    "    # 3. Derive Fiscal Period Columns for the ENTIRE dataset\n",
    "    created_date_source_col = next((col for col in ['Opportunity Created Date', 'Created Date'] if col in processed_df.columns and processed_df[col].notnull().any()), None)\n",
    "    if created_date_source_col:\n",
    "        processed_df['Fiscal Period - Created Date'] = processed_df[created_date_source_col].apply(lambda x: get_fiscal_quarter_info(x)[1])\n",
    "    if 'Close Date' in processed_df.columns and processed_df['Close Date'].notnull().any():\n",
    "        processed_df['Fiscal Period - Corrected'] = processed_df['Close Date'].apply(lambda x: get_fiscal_quarter_info(x)[1])\n",
    "\n",
    "    # 4. Create In-Quarter Booking Flag based on the full dataset\n",
    "    required_fiscal_cols = ['Fiscal Period - Corrected', 'Fiscal Period - Created Date']\n",
    "    if all(col in processed_df.columns and processed_df[col].notnull().any() for col in required_fiscal_cols):\n",
    "        processed_df['Inquarter Booking Flag'] = processed_df['Fiscal Period - Corrected'] == processed_df['Fiscal Period - Created Date']\n",
    "        print(\"Info: 'Inquarter Booking Flag' calculated on the full dataset.\")\n",
    "    else:\n",
    "        print(\"Warning: Could not create 'Inquarter Booking Flag' due to missing fiscal period columns.\")\n",
    "        processed_df['Inquarter Booking Flag'] = False\n",
    "\n",
    "    # 5. Identify Opportunity ID Column\n",
    "    potential_opp_id_cols = ['Opportunity ID', 'SFDC ID 18 Digit', 'Opportunity: ID'] \n",
    "    OPP_ID_COL_NAME = next((col for col in potential_opp_id_cols if col in processed_df.columns), None)\n",
    "    if OPP_ID_COL_NAME is None and not processed_df.empty:\n",
    "        processed_df['opportunity_pseudo_id'] = range(len(processed_df))\n",
    "        OPP_ID_COL_NAME = 'opportunity_pseudo_id'\n",
    "    \n",
    "    # 6. Filter out unwanted segments\n",
    "    segments_to_exclude = ['Self Serve', 'Self Service', 'Unknown']\n",
    "    if 'Segment - historical' in processed_df.columns:\n",
    "        original_count = len(processed_df)\n",
    "        processed_df = processed_df[~processed_df['Segment - historical'].isin(segments_to_exclude)]\n",
    "        filtered_count = len(processed_df)\n",
    "        excluded_count = original_count - filtered_count\n",
    "        print(f\"Segment filtering: Excluded {excluded_count} records from segments: {segments_to_exclude}\")\n",
    "        print(f\"Remaining records after segment filtering: {filtered_count}\")\n",
    "    else:\n",
    "        print(\"Warning: 'Segment - historical' column not found. Skipping segment filtering.\")\n",
    "    \n",
    "    # 7. Create inq_df (from the filtered dataset)\n",
    "    inq_df = processed_df[processed_df['Inquarter Booking Flag']].copy() if 'Inquarter Booking Flag' in processed_df.columns else pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\nPreprocessing complete on filtered dataset. Shape: {processed_df.shape}\")\n",
    "    if OPP_ID_COL_NAME: print(f\"Opportunity ID Column used: {OPP_ID_COL_NAME}\")\n",
    "    if not inq_df.empty: print(f\"In-Quarter DataFrame (inq_df) shape: {inq_df.shape}\")\n",
    "else:\n",
    "    print(\"Preprocessing skipped: Raw DataFrame is empty.\")\n",
    "\n",
    "# Create sf alias for sf_session to fix pipegen cell\n",
    "if 'sf_session' in locals():\n",
    "    sf = sf_session\n",
    "\n",
    "print(\"Cell 5: Data Preprocessing with Segment Filtering - Completed Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2chlbjv38bt",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T00:01:48.057867Z",
     "iopub.status.busy": "2025-07-03T00:01:48.057763Z",
     "iopub.status.idle": "2025-07-03T00:01:55.054303Z",
     "shell.execute_reply": "2025-07-03T00:01:55.053641Z",
     "shell.execute_reply.started": "2025-07-03T00:01:48.057853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Date Enhancement Using Production Module ---\n",
      "✅ processed_df found successfully - proceeding with date enhancement\n",
      "Enhancing date columns: ['Created Date', 'Close Date', 'SQO Date', 'SAO Date', 'Timestamp: Solution Validation']\n",
      "Converted Created Date to datetime\n",
      "Converted Close Date to datetime\n",
      "Converted SQO Date to datetime\n",
      "Converted SAO Date to datetime\n",
      "Converted Timestamp: Solution Validation to datetime\n",
      "Processing date breakdowns for: Created Date\n",
      "Added breakdown columns for Created Date: ['Created Date_Quarter', 'Created Date_Week_of_Quarter', 'Created Date_Month', 'Created Date_Day_of_Week', 'Created Date_Day_Name']\n",
      "Processing date breakdowns for: Close Date\n",
      "Added breakdown columns for Close Date: ['Close Date_Quarter', 'Close Date_Week_of_Quarter', 'Close Date_Month', 'Close Date_Day_of_Week', 'Close Date_Day_Name']\n",
      "Processing date breakdowns for: SQO Date\n",
      "Added breakdown columns for SQO Date: ['SQO Date_Quarter', 'SQO Date_Week_of_Quarter', 'SQO Date_Month', 'SQO Date_Day_of_Week', 'SQO Date_Day_Name']\n",
      "Processing date breakdowns for: SAO Date\n",
      "Added breakdown columns for SAO Date: ['SAO Date_Quarter', 'SAO Date_Week_of_Quarter', 'SAO Date_Month', 'SAO Date_Day_of_Week', 'SAO Date_Day_Name']\n",
      "Processing date breakdowns for: Timestamp: Solution Validation\n",
      "Added breakdown columns for Timestamp: Solution Validation: ['Timestamp: Solution Validation_Quarter', 'Timestamp: Solution Validation_Week_of_Quarter', 'Timestamp: Solution Validation_Month', 'Timestamp: Solution Validation_Day_of_Week', 'Timestamp: Solution Validation_Day_Name']\n",
      "Updating Fiscal Period - Corrected using production quarter logic\n",
      "Updated Fiscal Period - Corrected column\n",
      "Adding day of quarter information for Close Date\n",
      "Added: Day_of_Quarter, Total_Days_in_Quarter, Pct_Day, Pct_Day_Bin, Quarter\n",
      "Enhanced DataFrame now has 66 columns\n",
      "--- Finished Date Enhancement System Implementation ---\n"
     ]
    }
   ],
   "source": [
    "# ## Date Enhancement System (Using Production Module)\n",
    "#\n",
    "# This section uses the modular date_utils.py to enhance date columns with production logic.\n",
    "\n",
    "from date_utils import enhance_dataframe_dates\n",
    "\n",
    "print(\"--- Date Enhancement Using Production Module ---\")\n",
    "\n",
    "# Safety check: Ensure processed_df exists before proceeding\n",
    "if 'processed_df' not in locals() or processed_df.empty:\n",
    "    print(\"❌ ERROR: processed_df is not available. Please run the Data Preprocessing cell first.\")\n",
    "    print(\"   The Date Enhancement system requires processed_df to be created from the raw Salesforce data.\")\n",
    "else:\n",
    "    print(\"✅ processed_df found successfully - proceeding with date enhancement\")\n",
    "    \n",
    "    # Use the modular function to enhance dates\n",
    "    processed_df = enhance_dataframe_dates(processed_df)\n",
    "    \n",
    "    print(\"--- Finished Date Enhancement System Implementation ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "qnz905c5gxf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T00:01:55.057378Z",
     "iopub.status.busy": "2025-07-03T00:01:55.057226Z",
     "iopub.status.idle": "2025-07-03T00:02:45.591776Z",
     "shell.execute_reply": "2025-07-03T00:02:45.591410Z",
     "shell.execute_reply.started": "2025-07-03T00:01:55.057366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 ARR CHANGE HISTORY & PIPELINE GENERATION ANALYSIS\n",
      "============================================================\n",
      "✅ Salesforce connection found\n",
      "📊 Loading ARR change history from report: 00OUO000009jhTS2AY\n",
      "✅ Successfully loaded 28849 records from ARR change history report\n",
      "🔗 Using stage timestamp data from processed_df (14570 opportunities)\n",
      "🔄 Processing ARR change history for pipeline generation analysis...\n",
      "📋 Available columns in ARR change history: ['SFDC ID 18 Digit', 'ARR Change', 'Old Value', 'Field / Event', 'New Value', 'Edit Date']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bchen/Documents/Master reporting automation/pipegen_analyzer.py:89: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  history_df[col] = pd.to_datetime(history_df[col], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Processing 28849 ARR change history records...\n",
      "🔄 Consolidating multiple daily edits per opportunity...\n",
      "📉 Consolidated to 13796 records after removing multiple daily edits\n",
      "🔗 Merging ARR change history with stage timestamp data...\n",
      "📋 Available columns in stage timestamp data: ['SFDC ID 18 Digit', 'Opportunity Name', 'Account Name', 'Account 18 Digit ID', 'Opportunity Owner', 'Credited Rep', 'Credited Rep Role', 'Owner User Segment', 'Owner Role', 'Stage', 'Forecast Category', 'Fiscal Period - Created Date', 'Fiscal Period - Corrected', 'Created Date', 'SQO Date', 'SAO Date', 'Timestamp: Solution Validation', 'Close Date', 'Source', 'Bookings Type', 'Type', 'Primary Partner', 'Won', 'Age', 'Days Since Qualified', 'ARR Change', 'Workplace ARR', 'VR ARR', 'Emergency Notification ARR', 'Deliveries ARR', 'Priority Support ARR', 'Segment - historical', 'Created By Group - Historical', 'Created By Role - Historical', 'Opportunity Owner: Active', 'Inquarter Booking Flag', 'Created Date_Quarter', 'Created Date_Week_of_Quarter', 'Created Date_Month', 'Created Date_Day_of_Week', 'Created Date_Day_Name', 'Close Date_Quarter', 'Close Date_Week_of_Quarter', 'Close Date_Month', 'Close Date_Day_of_Week', 'Close Date_Day_Name', 'SQO Date_Quarter', 'SQO Date_Week_of_Quarter', 'SQO Date_Month', 'SQO Date_Day_of_Week', 'SQO Date_Day_Name', 'SAO Date_Quarter', 'SAO Date_Week_of_Quarter', 'SAO Date_Month', 'SAO Date_Day_of_Week', 'SAO Date_Day_Name', 'Timestamp: Solution Validation_Quarter', 'Timestamp: Solution Validation_Week_of_Quarter', 'Timestamp: Solution Validation_Month', 'Timestamp: Solution Validation_Day_of_Week', 'Timestamp: Solution Validation_Day_Name', 'Day_of_Quarter', 'Total_Days_in_Quarter', 'Pct_Day', 'Pct_Day_Bin', 'Quarter']\n",
      "📊 Created ARR lookup for 7067 opportunities\n",
      "📊 Stage timestamp opportunities: 14570\n",
      "📊 ARR change history opportunities: 7067\n",
      "📊 Matching opportunities: 6431\n",
      "🔍 Debug 006UO00000G2NXCYA3: SAO Date 2024-11-08 00:00:00 (type: <class 'pandas._libs.tslibs.timestamps.Timestamp'>)\n",
      "🔍 Debug 006UO00000G2NXCYA3: Edit dates sample: [Timestamp('2024-11-08 03:27:00')]\n",
      "🔍 Debug 006UO00000G2NXCYA3: New Value sample: [nan]\n",
      "🔍 Debug 006UO00000G2NXCYA3: Using master report ARR Change: 1177.2\n",
      "🔍 Debug 006UO00000NwzU5YAJ: SAO Date 2025-04-28 00:00:00 (type: <class 'pandas._libs.tslibs.timestamps.Timestamp'>)\n",
      "🔍 Debug 006UO00000NwzU5YAJ: Edit dates sample: [Timestamp('2025-04-28 01:22:00'), Timestamp('2025-04-29 03:26:00')]\n",
      "🔍 Debug 006UO00000NwzU5YAJ: New Value sample: [nan, nan]\n",
      "🔍 Debug 006UO00000NwzU5YAJ: Using master report ARR Change: 1177.2\n",
      "🔍 Debug 006UO00000GNIw0YAH: SAO Date 2024-11-19 00:00:00 (type: <class 'pandas._libs.tslibs.timestamps.Timestamp'>)\n",
      "🔍 Debug 006UO00000GNIw0YAH: Edit dates sample: [Timestamp('2024-11-19 14:33:00'), Timestamp('2024-11-22 09:13:00')]\n",
      "🔍 Debug 006UO00000GNIw0YAH: New Value sample: [nan, nan]\n",
      "🔍 Debug 006UO00000GNIw0YAH: Using master report ARR Change: 7177.2\n",
      "📊 Successfully processed 6431 opportunities with stage timestamp data\n",
      "✅ Pipeline generation analysis complete!\n",
      "📊 Generated pipeline data for 6082 opportunities\n",
      "💰 Total pipeline generated: $80,330,799\n",
      "\n",
      "📋 PIPELINE GENERATION SUMMARY\n",
      "========================================\n",
      "Total Opportunities: 6,082\n",
      "Total Pipeline Generated: $80,330,799\n",
      "Average Pipeline per Opp: $13,208\n",
      "\n",
      "📊 BY SEGMENT:\n",
      "            Total Pipegen ARR  Count  Avg Pipegen ARR\n",
      "Segment                                              \n",
      "Enterprise      44,524,863.00   1949        22,845.00\n",
      "Mid Market      17,176,075.00   1571        10,933.00\n",
      "SMB             18,629,860.00   2562         7,272.00\n",
      "\n",
      "📅 BY QUARTER:\n",
      "                  Total Pipegen ARR  Count\n",
      "SAO Date_Quarter                          \n",
      "1                     27,834,993.00   1871\n",
      "2                     22,651,394.00   1451\n",
      "3                     14,336,398.00   1268\n",
      "4                     15,508,014.00   1492\n",
      "\n",
      "🎯 Pipegen DataFrame created with 6082 opportunities\n",
      "💰 Total pipeline generated: $80,330,799\n",
      "\n",
      "📋 Sample Pipegen Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SFDC ID 18 Digit</th>\n",
       "      <th>Pipegen ARR</th>\n",
       "      <th>SAO Date_Quarter</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>006UO00000G2NXCYA3</td>\n",
       "      <td>1,177.20</td>\n",
       "      <td>4</td>\n",
       "      <td>SMB</td>\n",
       "      <td>Closed Won</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>006UO00000NwzU5YAJ</td>\n",
       "      <td>1,177.20</td>\n",
       "      <td>1</td>\n",
       "      <td>SMB</td>\n",
       "      <td>Closed Lost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006UO00000GNIw0YAH</td>\n",
       "      <td>7,177.20</td>\n",
       "      <td>4</td>\n",
       "      <td>SMB</td>\n",
       "      <td>Closed Lost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>006UO000001VYJQYA4</td>\n",
       "      <td>1,236.29</td>\n",
       "      <td>4</td>\n",
       "      <td>Enterprise</td>\n",
       "      <td>Closed Lost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>006UO000002c1QFYAY</td>\n",
       "      <td>3,847.18</td>\n",
       "      <td>4</td>\n",
       "      <td>Enterprise</td>\n",
       "      <td>Closed Lost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>006UO000005N1a1YAC</td>\n",
       "      <td>274.80</td>\n",
       "      <td>1</td>\n",
       "      <td>Enterprise</td>\n",
       "      <td>Closed Won</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>006UO00000JZiZXYA1</td>\n",
       "      <td>6,316.81</td>\n",
       "      <td>1</td>\n",
       "      <td>Enterprise</td>\n",
       "      <td>Closed Lost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0065b000013y7dTAAQ</td>\n",
       "      <td>2,700.00</td>\n",
       "      <td>3</td>\n",
       "      <td>SMB</td>\n",
       "      <td>Closed Won</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0065b000013y0VkAAI</td>\n",
       "      <td>2,199.04</td>\n",
       "      <td>3</td>\n",
       "      <td>Mid Market</td>\n",
       "      <td>Closed Won</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>006UO00000FSz7xYAD</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>3</td>\n",
       "      <td>Mid Market</td>\n",
       "      <td>Closed Lost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     SFDC ID 18 Digit  Pipegen ARR  SAO Date_Quarter     Segment        Stage\n",
       "0  006UO00000G2NXCYA3     1,177.20                 4         SMB   Closed Won\n",
       "1  006UO00000NwzU5YAJ     1,177.20                 1         SMB  Closed Lost\n",
       "2  006UO00000GNIw0YAH     7,177.20                 4         SMB  Closed Lost\n",
       "3  006UO000001VYJQYA4     1,236.29                 4  Enterprise  Closed Lost\n",
       "4  006UO000002c1QFYAY     3,847.18                 4  Enterprise  Closed Lost\n",
       "5  006UO000005N1a1YAC       274.80                 1  Enterprise   Closed Won\n",
       "6  006UO00000JZiZXYA1     6,316.81                 1  Enterprise  Closed Lost\n",
       "7  0065b000013y7dTAAQ     2,700.00                 3         SMB   Closed Won\n",
       "8  0065b000013y0VkAAI     2,199.04                 3  Mid Market   Closed Won\n",
       "9  006UO00000FSz7xYAD        -0.06                 3  Mid Market  Closed Lost"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ## ARR Change History Processing and Pipeline Generation Analysis (Direct Load)\n",
    "#\n",
    "# This section loads ARR change history directly from Salesforce and processes it for pipeline generation analysis.\n",
    "\n",
    "from pipegen_analyzer import process_arr_change_history, display_pipegen_summary, calculate_pipegen_6_row_analysis\n",
    "\n",
    "print(\"🚀 ARR CHANGE HISTORY & PIPELINE GENERATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if we have Salesforce connection\n",
    "if 'sf' in locals():\n",
    "    print(\"✅ Salesforce connection found\")\n",
    "    \n",
    "    try:\n",
    "        # Direct report loading approach with correct report ID\n",
    "        sf_instance = 'https://envoy.my.salesforce.com/'  # Your Salesforce Instance URL\n",
    "        reportId = '00OUO000009jhTS2AY'  # ARR change history report ID (corrected)\n",
    "        export = '?isdtp=p1&export=1&enc=UTF-8&xf=csv'\n",
    "        sfUrl = sf_instance + reportId + export\n",
    "        \n",
    "        print(f\"📊 Loading ARR change history from report: {reportId}\")\n",
    "        response = requests.get(sfUrl, headers=sf.headers, cookies={'sid': sf.session_id})\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        download_report = response.content.decode('utf-8')\n",
    "        raw_pipegen_df = pd.read_csv(StringIO(download_report))\n",
    "        \n",
    "        print(f\"✅ Successfully loaded {len(raw_pipegen_df)} records from ARR change history report\")\n",
    "        \n",
    "        # Process the raw data for pipeline generation analysis with stage timestamp data\n",
    "        stage_timestamp_data = processed_df if 'processed_df' in locals() and not processed_df.empty else None\n",
    "        if stage_timestamp_data is not None:\n",
    "            print(f\"🔗 Using stage timestamp data from processed_df ({len(stage_timestamp_data)} opportunities)\")\n",
    "        else:\n",
    "            print(\"⚠️ No stage timestamp data available (processed_df not found)\")\n",
    "            \n",
    "        pipegen_df = process_arr_change_history(raw_pipegen_df, stage_timestamp_data)\n",
    "        \n",
    "        if not pipegen_df.empty:\n",
    "            # Display summary\n",
    "            display_pipegen_summary(pipegen_df)\n",
    "            \n",
    "            print(f\"\\n🎯 Pipegen DataFrame created with {len(pipegen_df)} opportunities\")\n",
    "            print(f\"💰 Total pipeline generated: ${pipegen_df['Pipegen ARR'].sum():,.0f}\")\n",
    "            \n",
    "            # Display sample data\n",
    "            print(f\"\\n📋 Sample Pipegen Data:\")\n",
    "            sample_cols = ['SFDC ID 18 Digit', 'Pipegen ARR', 'SAO Date_Quarter', 'Segment', 'Stage']\n",
    "            available_cols = [col for col in sample_cols if col in pipegen_df.columns]\n",
    "            if available_cols:\n",
    "                display(pipegen_df[available_cols].head(10))\n",
    "            \n",
    "        else:\n",
    "            print(\"❌ No pipeline generation data could be processed after loading report\")\n",
    "            pipegen_df = pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading ARR change history report: {e}\")\n",
    "        print(\"📝 Continuing analysis without Pipegen data - all other analyses will work\")\n",
    "        pipegen_df = pd.DataFrame()\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No Salesforce connection available. Please run the Salesforce authentication cell first.\")\n",
    "    pipegen_df = pd.DataFrame()  # Create empty DataFrame as fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "euoujg5vpc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T00:02:45.592453Z",
     "iopub.status.busy": "2025-07-03T00:02:45.592347Z",
     "iopub.status.idle": "2025-07-03T00:02:45.627781Z",
     "shell.execute_reply": "2025-07-03T00:02:45.627411Z",
     "shell.execute_reply.started": "2025-07-03T00:02:45.592444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 PIPEGEN QUARTERLY ANALYSIS - 6 ROW STRUCTURE\n",
      "=======================================================\n",
      "📊 Sample fiscal period mapping from SAO Date:     SAO Date  SAO Date_Quarter Fiscal Period\n",
      "0 2024-11-08                 4      FY2025Q4\n",
      "1 2025-04-28                 1      FY2026Q1\n",
      "2 2024-11-19                 4      FY2025Q4\n",
      "3 2023-12-05                 4      FY2024Q4\n",
      "4 2024-01-04                 4      FY2024Q4\n",
      "📊 Found fiscal periods in pipegen data: ['FY2023Q2', 'FY2023Q3', 'FY2023Q4', 'FY2024Q1', 'FY2024Q2', 'FY2024Q3', 'FY2024Q4', 'FY2025Q1', 'FY2025Q2', 'FY2025Q3', 'FY2025Q4', 'FY2026Q1', 'FY2026Q2']\n",
      "📊 Available Bookings Type values in pipegen data: ['New Business' 'Expansion']\n",
      "📊 Looking for Bookings Type values: 'Expansion' and 'New Business'\n",
      "📊 Expansion data found: 3697 opportunities\n",
      "📊 New Business data found: 2383 opportunities\n",
      "\n",
      "✅ Pipegen quarterly analysis completed successfully!\n",
      "\n",
      "Displaying: Pipeline Generation Summary (Fiscal Period Breakdown)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FY2023Q2</th>\n",
       "      <th>FY2023Q3</th>\n",
       "      <th>FY2023Q4</th>\n",
       "      <th>FY2024Q1</th>\n",
       "      <th>FY2024Q2</th>\n",
       "      <th>FY2024Q3</th>\n",
       "      <th>FY2024Q4</th>\n",
       "      <th>FY2025Q1</th>\n",
       "      <th>FY2025Q2</th>\n",
       "      <th>FY2025Q3</th>\n",
       "      <th>FY2025Q4</th>\n",
       "      <th>FY2026Q1</th>\n",
       "      <th>FY2026Q2</th>\n",
       "      <th>QoQ Change</th>\n",
       "      <th>vs Last 4Q Avg</th>\n",
       "      <th>YoY Change</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Expansion</th>\n",
       "      <td>1,000.00</td>\n",
       "      <td>38,470.38</td>\n",
       "      <td>133,784.00</td>\n",
       "      <td>192,196.39</td>\n",
       "      <td>490,672.57</td>\n",
       "      <td>2,758,510.75</td>\n",
       "      <td>4,095,681.41</td>\n",
       "      <td>6,062,623.75</td>\n",
       "      <td>5,465,500.76</td>\n",
       "      <td>4,257,817.47</td>\n",
       "      <td>3,367,287.30</td>\n",
       "      <td>8,606,602.93</td>\n",
       "      <td>5,703,170.50</td>\n",
       "      <td>-33.73</td>\n",
       "      <td>5.14</td>\n",
       "      <td>4.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Business</th>\n",
       "      <td>299,064.00</td>\n",
       "      <td>56,598.08</td>\n",
       "      <td>46,583.52</td>\n",
       "      <td>1,005,257.16</td>\n",
       "      <td>829,984.48</td>\n",
       "      <td>3,820,622.40</td>\n",
       "      <td>3,830,641.33</td>\n",
       "      <td>4,664,216.67</td>\n",
       "      <td>4,641,843.93</td>\n",
       "      <td>3,404,378.52</td>\n",
       "      <td>3,976,450.36</td>\n",
       "      <td>7,304,096.56</td>\n",
       "      <td>5,210,567.71</td>\n",
       "      <td>-28.66</td>\n",
       "      <td>7.84</td>\n",
       "      <td>12.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total by Period</th>\n",
       "      <td>300,064.00</td>\n",
       "      <td>95,068.46</td>\n",
       "      <td>180,367.52</td>\n",
       "      <td>1,197,453.55</td>\n",
       "      <td>1,320,657.05</td>\n",
       "      <td>6,579,133.15</td>\n",
       "      <td>7,926,322.74</td>\n",
       "      <td>10,726,840.42</td>\n",
       "      <td>10,107,344.69</td>\n",
       "      <td>7,662,195.99</td>\n",
       "      <td>7,343,737.66</td>\n",
       "      <td>15,910,699.49</td>\n",
       "      <td>10,913,738.21</td>\n",
       "      <td>-31.41</td>\n",
       "      <td>6.41</td>\n",
       "      <td>7.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  FY2023Q2  FY2023Q3   FY2023Q4     FY2024Q1     FY2024Q2     FY2024Q3     FY2024Q4      FY2025Q1      FY2025Q2     FY2025Q3     FY2025Q4      FY2026Q1      FY2026Q2  QoQ Change  vs Last 4Q Avg  YoY Change\n",
       "Metric                                                                                                                                                                                                                       \n",
       "Expansion         1,000.00 38,470.38 133,784.00   192,196.39   490,672.57 2,758,510.75 4,095,681.41  6,062,623.75  5,465,500.76 4,257,817.47 3,367,287.30  8,606,602.93  5,703,170.50      -33.73            5.14        4.35\n",
       "New Business    299,064.00 56,598.08  46,583.52 1,005,257.16   829,984.48 3,820,622.40 3,830,641.33  4,664,216.67  4,641,843.93 3,404,378.52 3,976,450.36  7,304,096.56  5,210,567.71      -28.66            7.84       12.25\n",
       "Total by Period 300,064.00 95,068.46 180,367.52 1,197,453.55 1,320,657.05 6,579,133.15 7,926,322.74 10,726,840.42 10,107,344.69 7,662,195.99 7,343,737.66 15,910,699.49 10,913,738.21      -31.41            6.41        7.98"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Summary Statistics:\n"
     ]
    }
   ],
   "source": [
    "# ## Pipegen Analysis - 6 Row Structure with Quarterly Breakdown\n",
    "#\n",
    "# This section performs 6-row pipeline generation analysis with quarterly columns and comparison metrics.\n",
    "\n",
    "from metrics_calculator import calculate_pipegen_quarterly_summary, add_pipegen_comparison_columns\n",
    "\n",
    "print(\"🎯 PIPEGEN QUARTERLY ANALYSIS - 6 ROW STRUCTURE\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Check if pipegen_df exists from previous cell\n",
    "if 'pipegen_df' in locals() and not pipegen_df.empty:\n",
    "    # Calculate quarterly summary with 6-row structure using fiscal periods\n",
    "    # Use 'Bookings Type' column which contains 'Expansion' and 'New Business' values\n",
    "    pipegen_quarterly_summary = calculate_pipegen_quarterly_summary(\n",
    "        pipegen_df, \n",
    "        bookings_type_col='Bookings Type', \n",
    "        start_fiscal_year=START_FISCAL_YEAR_FOR_ANALYSIS if 'START_FISCAL_YEAR_FOR_ANALYSIS' in locals() else 2023\n",
    "    )\n",
    "    \n",
    "    if not pipegen_quarterly_summary.empty:\n",
    "        # Add comparison columns (QoQ, vs Last 4Q Avg, YoY)\n",
    "        pipegen_summary_with_comparisons = add_pipegen_comparison_columns(pipegen_quarterly_summary)\n",
    "        \n",
    "        print(\"\\n✅ Pipegen quarterly analysis completed successfully!\")\n",
    "        print(f\"\\nDisplaying: Pipeline Generation Summary (Fiscal Period Breakdown)\")\n",
    "        \n",
    "        # Display the full quarterly breakdown with comparisons\n",
    "        display(pipegen_summary_with_comparisons)\n",
    "        \n",
    "        print(\"\\n📊 Summary Statistics:\")\n",
    "        if 'Total Pipeline Generated' in pipegen_summary_with_comparisons.index:\n",
    "            total_pipegen = pipegen_summary_with_comparisons.loc['Total Pipeline Generated'].iloc[0]\n",
    "            print(f\"Total Pipeline Generated: ${total_pipegen:,.0f}\")\n",
    "            \n",
    "            # Get most recent quarter value\n",
    "            quarter_cols = [col for col in pipegen_summary_with_comparisons.columns \n",
    "                          if isinstance(col, str) and col.startswith('FY') and 'Q' in col]\n",
    "            if quarter_cols:\n",
    "                latest_quarter = sorted(quarter_cols)[-1]\n",
    "                latest_quarter_total = pipegen_summary_with_comparisons.loc['Total InQuarter', latest_quarter]\n",
    "                print(f\"Latest Quarter ({latest_quarter}) Pipeline: ${latest_quarter_total:,.0f}\")\n",
    "    else:\n",
    "        print(\"❌ Could not generate pipegen quarterly summary - no data available\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Pipegen data not found. Please run the ARR Change History cell first.\")\n",
    "    print(\"   This analysis requires pipegen_df to be created from the Salesforce ARR change history.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ux6xrmnyr1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T00:02:45.628466Z",
     "iopub.status.busy": "2025-07-03T00:02:45.628386Z",
     "iopub.status.idle": "2025-07-03T00:02:45.633456Z",
     "shell.execute_reply": "2025-07-03T00:02:45.633047Z",
     "shell.execute_reply.started": "2025-07-03T00:02:45.628458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 COMPREHENSIVE TREND ANALYSIS WITH PIPEGEN SUPPORT\n",
      "============================================================\n",
      "❌ Missing required variables: ['processed_df', 'OPP_ID_COL_NAME', 'START_FISCAL_YEAR_FOR_ANALYSIS']\n",
      "   Please run the Data Preprocessing cell first.\n",
      "\n",
      "--- Trend Analysis Module Implementation Complete ---\n"
     ]
    }
   ],
   "source": [
    "# ## Enhanced Comprehensive Trend Analysis (Using Module)\n",
    "#\n",
    "# This section performs comprehensive trend analysis including Pipeline Generation metrics using the modular approach.\n",
    "\n",
    "from trend_analyzer import perform_trend_analysis, save_trend_results\n",
    "\n",
    "print(\"🚀 COMPREHENSIVE TREND ANALYSIS WITH PIPEGEN SUPPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if required data exists\n",
    "required_vars = ['processed_df', 'OPP_ID_COL_NAME', 'START_FISCAL_YEAR_FOR_ANALYSIS']\n",
    "missing_vars = [var for var in required_vars if var not in locals()]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"❌ Missing required variables: {missing_vars}\")\n",
    "    print(\"   Please run the Data Preprocessing cell first.\")\n",
    "else:\n",
    "    # Prepare inq_df if it exists\n",
    "    inq_df_for_analysis = inq_df if 'inq_df' in locals() and not inq_df.empty else pd.DataFrame()\n",
    "    \n",
    "    # Prepare pipegen_df if it exists\n",
    "    pipegen_df_for_analysis = pipegen_df if 'pipegen_df' in locals() and not pipegen_df.empty else None\n",
    "    \n",
    "    # Define custom thresholds (optional - will use defaults if None)\n",
    "    custom_thresholds = {\n",
    "        'pct_change_threshold': 20.0,\n",
    "        'abs_change_threshold_arr': 100000,\n",
    "        'abs_change_threshold_opps': 10,\n",
    "        'abs_change_threshold_wr': 10.0,\n",
    "        'abs_change_threshold_asc': 30.0,\n",
    "        'abs_change_threshold_asp': 5000,\n",
    "        'abs_change_threshold_pipegen': 50000\n",
    "    }\n",
    "    \n",
    "    # Perform trend analysis\n",
    "    significant_changes_df = perform_trend_analysis(\n",
    "        processed_df=processed_df,\n",
    "        inq_df=inq_df_for_analysis,\n",
    "        opp_id_col_name=OPP_ID_COL_NAME,\n",
    "        start_fiscal_year=START_FISCAL_YEAR_FOR_ANALYSIS,\n",
    "        stage_won=STAGE_WON if 'STAGE_WON' in locals() else 'Closed Won',\n",
    "        pipegen_df=pipegen_df_for_analysis,\n",
    "        thresholds=custom_thresholds\n",
    "    )\n",
    "    \n",
    "    # Save results if any significant changes found\n",
    "    if not significant_changes_df.empty:\n",
    "        save_trend_results(significant_changes_df, 'enhanced_trends_summary_with_pipegen.csv')\n",
    "        print(\"\\n🎉 Enhanced trend analysis with Pipegen support completed!\")\n",
    "    else:\n",
    "        print(\"\\n📊 Trend analysis completed - no significant changes detected.\")\n",
    "\n",
    "print(\"\\n--- Trend Analysis Module Implementation Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04063431-6a33-46b0-8635-c3ebcb97e95e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3da59685-b69b-40db-9ece-55819a2a56d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T00:02:45.634007Z",
     "iopub.status.busy": "2025-07-03T00:02:45.633890Z",
     "iopub.status.idle": "2025-07-03T00:02:45.927515Z",
     "shell.execute_reply": "2025-07-03T00:02:45.926050Z",
     "shell.execute_reply.started": "2025-07-03T00:02:45.633987Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asdfadfsadfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m asdfadfsadfs\n",
      "\u001b[0;31mNameError\u001b[0m: name 'asdfadfsadfs' is not defined"
     ]
    }
   ],
   "source": [
    "asdfadfsadfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d8a3e4-07f9-4e55-831b-457dc22c2941",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-03T00:02:45.927793Z",
     "iopub.status.idle": "2025-07-03T00:02:45.927921Z",
     "shell.execute_reply": "2025-07-03T00:02:45.927866Z",
     "shell.execute_reply.started": "2025-07-03T00:02:45.927861Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Data Preprocessing\n",
    "#\n",
    "# ### Key Steps:\n",
    "# 1.  **Date Conversion:** Convert all relevant date columns to a proper datetime format.\n",
    "# 2.  **ARR Processing:** Use `'ARR Change'` as the primary value metric. If it's not available, fall back to `'ARR'` or create a zero-value column to prevent errors. This ensures the notebook is robust.\n",
    "# 3.  **Fiscal Period Calculation:** For each opportunity, calculate its fiscal quarter based on both its `Created Date` and `Close Date` using the `get_fiscal_quarter_info` utility.\n",
    "# 4.  **In-Quarter Flag:** Create a boolean flag, `'Inquarter Booking Flag'`, which is `True` if an opportunity's creation and close fiscal quarters are the same.\n",
    "# 5.  **Opportunity ID:** Dynamically find the correct Opportunity ID column from a list of potential names to make the notebook more adaptable to different report formats.\n",
    "# 6.  **In-Quarter DataFrame (`inq_df`):** Create a separate DataFrame containing only the deals flagged as in-quarter bookings for focused analysis and forecasting.\n",
    "\n",
    "processed_df = pd.DataFrame()\n",
    "inq_df = pd.DataFrame()\n",
    "OPP_ID_COL_NAME = None \n",
    "\n",
    "if not raw_df.empty:\n",
    "    processed_df = raw_df.copy()\n",
    "\n",
    "    # 1. Convert Date Columns\n",
    "    date_cols_to_convert = ['Created Date', 'Close Date', 'Renewal Date', 'Opportunity Created Date']\n",
    "    for col in date_cols_to_convert:\n",
    "        if col in processed_df.columns:\n",
    "            processed_df[col] = pd.to_datetime(processed_df[col], errors='coerce')\n",
    "\n",
    "    # 2. Process ARR Column (Using 'ARR Change')\n",
    "    if 'ARR Change' in processed_df.columns:\n",
    "        processed_df['ARR Change'] = pd.to_numeric(processed_df['ARR Change'], errors='coerce').fillna(0)\n",
    "        print(\"Info: 'ARR Change' column processed as the primary value metric.\")\n",
    "    else:\n",
    "        # If 'ARR Change' is missing, fall back to 'ARR' or create a zero column.\n",
    "        if 'ARR' in processed_df.columns:\n",
    "            print(\"Warning: 'ARR Change' column not found. Falling back to using 'ARR'.\")\n",
    "            processed_df.rename(columns={'ARR': 'ARR Change'}, inplace=True) # Rename for consistency downstream\n",
    "            processed_df['ARR Change'] = pd.to_numeric(processed_df['ARR Change'], errors='coerce').fillna(0)\n",
    "        else:\n",
    "            print(\"CRITICAL WARNING: Neither 'ARR Change' nor 'ARR' column found. Creating 'ARR Change' with zeros.\")\n",
    "            processed_df['ARR Change'] = 0\n",
    "\n",
    "    # 3. Derive Fiscal Period Columns for the ENTIRE dataset\n",
    "    created_date_source_col = next((col for col in ['Opportunity Created Date', 'Created Date'] if col in processed_df.columns and processed_df[col].notnull().any()), None)\n",
    "    if created_date_source_col:\n",
    "        processed_df['Fiscal Period - Created Date'] = processed_df[created_date_source_col].apply(lambda x: get_fiscal_quarter_info(x)[1])\n",
    "    if 'Close Date' in processed_df.columns and processed_df['Close Date'].notnull().any():\n",
    "        processed_df['Fiscal Period - Corrected'] = processed_df['Close Date'].apply(lambda x: get_fiscal_quarter_info(x)[1])\n",
    "\n",
    "    # 4. Create In-Quarter Booking Flag based on the full dataset\n",
    "    required_fiscal_cols = ['Fiscal Period - Corrected', 'Fiscal Period - Created Date']\n",
    "    if all(col in processed_df.columns and processed_df[col].notnull().any() for col in required_fiscal_cols):\n",
    "        processed_df['Inquarter Booking Flag'] = processed_df['Fiscal Period - Corrected'] == processed_df['Fiscal Period - Created Date']\n",
    "        print(\"Info: 'Inquarter Booking Flag' calculated on the full dataset.\")\n",
    "    else:\n",
    "        print(\"Warning: Could not create 'Inquarter Booking Flag' due to missing fiscal period columns.\")\n",
    "        processed_df['Inquarter Booking Flag'] = False\n",
    "\n",
    "    # 5. Identify Opportunity ID Column\n",
    "    potential_opp_id_cols = ['Opportunity ID', 'SFDC ID 18 Digit', 'Opportunity: ID'] \n",
    "    OPP_ID_COL_NAME = next((col for col in potential_opp_id_cols if col in processed_df.columns), None)\n",
    "    if OPP_ID_COL_NAME is None and not processed_df.empty:\n",
    "        processed_df['opportunity_pseudo_id'] = range(len(processed_df))\n",
    "        OPP_ID_COL_NAME = 'opportunity_pseudo_id'\n",
    "    \n",
    "    # 6. Create inq_df (still from the full dataset)\n",
    "    inq_df = processed_df[processed_df['Inquarter Booking Flag']].copy() if 'Inquarter Booking Flag' in processed_df.columns else pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\nPreprocessing complete on full raw dataset. Shape: {processed_df.shape}\")\n",
    "    if OPP_ID_COL_NAME: print(f\"Opportunity ID Column used: {OPP_ID_COL_NAME}\")\n",
    "    if not inq_df.empty: print(f\"In-Quarter DataFrame (inq_df) shape: {inq_df.shape}\")\n",
    "else:\n",
    "    print(\"Preprocessing skipped: Raw DataFrame is empty.\")\n",
    "\n",
    "# ## Trend Analysis Functions\n",
    "#\n",
    "# This cell defines all necessary functions for calculating metrics and identifying significant trends. This modular approach allows for flexible comparisons (QoQ, YoY, etc.) without duplicating code.\n",
    "\n",
    "def calculate_quarterly_metrics(df_full, opp_id_col_name, group_by_cols=None):\n",
    "    \"\"\"Calculates all key sales metrics on a quarterly basis, with optional grouping.\"\"\"\n",
    "    if df_full.empty: return pd.DataFrame()\n",
    "    required_cols = ['Fiscal Period - Corrected', 'ARR Change', 'Stage', 'Created Date', 'Close Date', opp_id_col_name]\n",
    "    if not all(col in df_full.columns for col in required_cols):\n",
    "        print(f\"Warning: Missing one or more required columns in calculate_quarterly_metrics. Needed: {required_cols}\")\n",
    "        return pd.DataFrame()\n",
    "    df_copy = df_full.copy()\n",
    "    df_won = df_copy[df_copy['Stage'] == STAGE_WON].copy()\n",
    "    if not df_won.empty:\n",
    "        df_won.loc[:, 'Sales Cycle Days'] = (df_won['Close Date'] - df_won['Created Date']).dt.days\n",
    "        df_copy = df_copy.merge(df_won[['Sales Cycle Days']], left_index=True, right_index=True, how='left')\n",
    "    else:\n",
    "        df_copy['Sales Cycle Days'] = pd.NA\n",
    "    grouping_cols = []\n",
    "    if group_by_cols:\n",
    "        grouping_cols.extend([group_by_cols] if isinstance(group_by_cols, str) else list(group_by_cols))\n",
    "    grouping_cols.append('Fiscal Period - Corrected')\n",
    "    agg_funcs = {\n",
    "        'Total ARR': ('ARR Change', 'sum'),\n",
    "        '# of Opps': (opp_id_col_name, 'nunique'),\n",
    "        '# of Won Opps': (opp_id_col_name, lambda x: x[df_copy.loc[x.index, 'Stage'] == STAGE_WON].nunique()),\n",
    "        'Total Relevant Opps for WR': (opp_id_col_name, lambda x: x[df_copy.loc[x.index, 'Stage'].isin([STAGE_WON, STAGE_LOST])].nunique()),\n",
    "        'Won ARR': ('ARR Change', lambda x: x[df_copy.loc[x.index, 'Stage'] == STAGE_WON].sum()),\n",
    "        'Avg Sales Cycle': ('Sales Cycle Days', 'mean')\n",
    "    }\n",
    "    quarterly_summary = df_copy.groupby(grouping_cols, observed=False).agg(**agg_funcs)\n",
    "    quarterly_summary['Avg Sales Price'] = (quarterly_summary['Won ARR'] / quarterly_summary['# of Won Opps'].replace(0, np.nan)).fillna(0)\n",
    "    quarterly_summary['Win Rate (Count)'] = (quarterly_summary['# of Won Opps'] / quarterly_summary['Total Relevant Opps for WR'].replace(0, np.nan) * 100).fillna(0)\n",
    "    final_metrics = ['Total ARR', '# of Opps', '# of Won Opps', 'Won ARR', 'Avg Sales Price', 'Win Rate (Count)', 'Avg Sales Cycle']\n",
    "    return quarterly_summary[final_metrics]\n",
    "\n",
    "def build_comparison_view(quarterly_metrics_df, current_q, comparison_q_data, comparison_label, group_tuple=None):\n",
    "    \"\"\"Constructs a side-by-side comparison DataFrame for a given metric set.\"\"\"\n",
    "    if group_tuple:\n",
    "        # For grouped data, select the group first, then the specific quarter\n",
    "        df_to_use = quarterly_metrics_df.loc[group_tuple]\n",
    "        if current_q not in df_to_use.index: return pd.DataFrame()\n",
    "        current_q_data = df_to_use.loc[current_q]\n",
    "    else:\n",
    "        # For overall data, the index is just the quarter\n",
    "        if current_q not in quarterly_metrics_df.index: return pd.DataFrame()\n",
    "        current_q_data = quarterly_metrics_df.loc[current_q]\n",
    "    comparison_view = pd.DataFrame({'Current Quarter': current_q_data, comparison_label: comparison_q_data}).T\n",
    "    abs_change = comparison_view.loc['Current Quarter'] - comparison_view.loc[comparison_label]\n",
    "    pct_change = (abs_change / comparison_view.loc[comparison_label].replace(0, np.nan) * 100)\n",
    "    comparison_view.loc['Absolute Change'] = abs_change\n",
    "    comparison_view.loc['Percent Change'] = pct_change\n",
    "    return comparison_view.T.fillna({'Avg Sales Cycle': pd.NA}).fillna(0)\n",
    "\n",
    "def identify_and_report_significant_changes(comparison_df, current_q, comparison_label, group_label=\"Overall\"):\n",
    "    \"\"\"Analyzes a comparison DataFrame and returns a list of significant findings based on thresholds.\"\"\"\n",
    "    significant_findings_list = []\n",
    "    if comparison_df.empty: return significant_findings_list\n",
    "    \n",
    "    METRICS_TO_ANALYZE = ['Total ARR', '# of Opps', '# of Won Opps', 'Avg Sales Price', 'Win Rate (Count)', 'Avg Sales Cycle']\n",
    "    for metric_name in METRICS_TO_ANALYZE:\n",
    "        if metric_name not in comparison_df.index: continue\n",
    "        row_data = comparison_df.loc[metric_name]\n",
    "        pct_val, absolute_val = row_data.get('Percent Change', 0), row_data.get('Absolute Change', 0)\n",
    "        is_sig_drop, is_sig_inc = False, False\n",
    "        abs_num, pct_num = pd.to_numeric(absolute_val, 'coerce'), pd.to_numeric(pct_val, 'coerce')\n",
    "\n",
    "        if metric_name == 'Total ARR':\n",
    "            is_sig_drop = (pct_num <= PCT_CHANGE_THRESHOLD_DROP and abs(abs_num or 0) >= ABS_ARR_CHANGE_THRESHOLD)\n",
    "            is_sig_inc = (pct_num >= PCT_CHANGE_THRESHOLD_INCREASE and abs(abs_num or 0) >= ABS_ARR_CHANGE_THRESHOLD)\n",
    "        elif metric_name in ['# of Opps', '# of Won Opps']:\n",
    "            is_sig_drop = (pct_num <= PCT_CHANGE_THRESHOLD_DROP and abs(abs_num or 0) >= ABS_OPP_CHANGE_THRESHOLD)\n",
    "            is_sig_inc = (pct_num >= PCT_CHANGE_THRESHOLD_INCREASE and abs(abs_num or 0) >= ABS_OPP_CHANGE_THRESHOLD)\n",
    "        elif metric_name == 'Avg Sales Price':\n",
    "            is_sig_drop = (pct_num <= PCT_CHANGE_THRESHOLD_DROP and abs(abs_num or 0) >= ASP_CHANGE_THRESHOLD)\n",
    "            is_sig_inc = (pct_num >= PCT_CHANGE_THRESHOLD_INCREASE and abs(abs_num or 0) >= ASP_CHANGE_THRESHOLD)\n",
    "        elif metric_name == 'Win Rate (Count)':\n",
    "            is_sig_drop = (abs_num <= -WIN_RATE_PCT_POINT_CHANGE_THRESHOLD)\n",
    "            is_sig_inc = (abs_num >= WIN_RATE_PCT_POINT_CHANGE_THRESHOLD)\n",
    "        elif metric_name == 'Avg Sales Cycle':\n",
    "            is_sig_drop = ((abs_num <= -SALES_CYCLE_DAY_CHANGE_THRESHOLD) or (pct_num <= -SALES_CYCLE_PCT_CHANGE_THRESHOLD))\n",
    "            is_sig_inc = ((abs_num >= SALES_CYCLE_DAY_CHANGE_THRESHOLD) or (pct_num >= SALES_CYCLE_PCT_CHANGE_THRESHOLD))\n",
    "        \n",
    "        if is_sig_drop or is_sig_inc:\n",
    "            significant_findings_list.append({\n",
    "                'Quarter': current_q, 'Comparison': comparison_label, 'Group': group_label, 'Metric': metric_name, \n",
    "                '% Change': f\"{pct_num:.2f}%\" if pd.notnull(pct_num) else \"N/A\", \n",
    "                'Abs Change': f\"{abs_num:,.2f}\" if pd.notnull(abs_num) else \"N/A\",\n",
    "                'Type': 'Drop/Decrease' if is_sig_drop else 'Increase'\n",
    "            })\n",
    "    return significant_findings_list\n",
    "\n",
    "print(\"Cell 7: All Trend Analysis Functions Defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c076fd04-f77a-42d9-884b-00d41b18624a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-03T00:02:45.928480Z",
     "iopub.status.idle": "2025-07-03T00:02:45.928639Z",
     "shell.execute_reply": "2025-07-03T00:02:45.928554Z",
     "shell.execute_reply.started": "2025-07-03T00:02:45.928548Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 1. --- ADDED --- Import the display function for rich output in notebooks\n",
    "from IPython.display import display\n",
    "\n",
    "# ## Main Summary Tables (Similar to CSV Output)\n",
    "#\n",
    "# **NOTE:** \"Total bookings\" is the sum of 'ARR Change' for 'Closed Won' deals.\n",
    "# \"Total Inquarter\" is the sum of 'ARR Change' for 'Closed Won' deals that were also created in the same quarter.\n",
    "\n",
    "# --- OPTIONAL BUT RECOMMENDED --- Configure pandas for better display\n",
    "# This improves readability of the numbers in your tables.\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "def add_comparison_columns(df):\n",
    "    \"\"\"\n",
    "    Add QoQ Change, vs Last 4 Quarter Average, and YoY Change columns\n",
    "    to the rightmost side of the dataframe\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    # Get fiscal quarter columns (exclude non-quarter columns)\n",
    "    quarter_cols = [col for col in df.columns if isinstance(col, str) and col.startswith('FY') and 'Q' in col]\n",
    "    quarter_cols.sort()\n",
    "    \n",
    "    if len(quarter_cols) < 2:\n",
    "        return df\n",
    "    \n",
    "    # Initialize comparison columns\n",
    "    qoq_change_col = 'QoQ Change'\n",
    "    vs_last4_avg_col = 'vs Last 4Q Avg'\n",
    "    yoy_change_col = 'YoY Change'\n",
    "    \n",
    "    # Get the most recent quarter (last column)\n",
    "    current_quarter = quarter_cols[-1]\n",
    "    \n",
    "    # Convert columns to numeric, replacing non-numeric values with NaN\n",
    "    for col in quarter_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # QoQ Change\n",
    "    if len(quarter_cols) >= 2:\n",
    "        prev_quarter = quarter_cols[-2]\n",
    "        qoq_values = ((df[current_quarter] - df[prev_quarter]) / df[prev_quarter].replace(0, np.nan) * 100)\n",
    "        df[qoq_change_col] = qoq_values.round(2)\n",
    "    else:\n",
    "        df[qoq_change_col] = np.nan\n",
    "    \n",
    "    # vs Last 4 Quarter Average\n",
    "    if len(quarter_cols) >= 5:\n",
    "        last_4_quarters = quarter_cols[-5:-1]  # Get 4 quarters before current\n",
    "        avg_last_4 = df[last_4_quarters].mean(axis=1)\n",
    "        vs_avg_values = ((df[current_quarter] - avg_last_4) / avg_last_4.replace(0, np.nan) * 100)\n",
    "        df[vs_last4_avg_col] = vs_avg_values.round(2)\n",
    "    else:\n",
    "        df[vs_last4_avg_col] = np.nan\n",
    "    \n",
    "    # YoY Change\n",
    "    # Find same quarter from previous year\n",
    "    current_year = int(current_quarter[2:6])\n",
    "    current_q = current_quarter[-2:]\n",
    "    prev_year_quarter = f\"FY{current_year-1}{current_q}\"\n",
    "    \n",
    "    if prev_year_quarter in quarter_cols:\n",
    "        yoy_values = ((df[current_quarter] - df[prev_year_quarter]) / df[prev_year_quarter].replace(0, np.nan) * 100)\n",
    "        df[yoy_change_col] = yoy_values.round(2)\n",
    "    else:\n",
    "        df[yoy_change_col] = np.nan\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"--- Attempting to Generate Main Summary Tables (ARR Focused) ---\")\n",
    "\n",
    "# Define the stage name for a won deal, consistent with Cell 2's configuration\n",
    "if 'STAGE_WON' not in locals(): STAGE_WON = 'Closed Won'\n",
    "\n",
    "# Ensure processed_df is valid and has necessary columns\n",
    "if not processed_df.empty and \\\n",
    "   all(col in processed_df.columns for col in ['Fiscal Period - Corrected', 'ARR Change', 'Bookings Type', 'Inquarter Booking Flag', 'Stage']):\n",
    "\n",
    "    # Create a DataFrame containing only \"booked\" (won) deals. This will be the basis for ALL bookings calculations.\n",
    "    booked_deals_df = processed_df[processed_df['Stage'] == STAGE_WON].copy()\n",
    "    \n",
    "    if booked_deals_df.empty:\n",
    "        print(\"WARNING: No deals with Stage 'Closed Won' found. All booking values will be zero.\")\n",
    "    \n",
    "    print(f\"\\n{'='*15} Generating Main Summary Table: By Bookings Type {'='*15}\")\n",
    "\n",
    "    # Create a DataFrame of deals that are BOTH in-quarter AND booked/won.\n",
    "    in_quarter_booked_deals_df = booked_deals_df[booked_deals_df['Inquarter Booking Flag'] == True].copy()\n",
    "    \n",
    "    if in_quarter_booked_deals_df.empty:\n",
    "        print(\"Warning: No 'Closed Won' deals were found within the in-quarter dataset.\")\n",
    "    \n",
    "    # Calculate In-Quarter ARR using the filtered in_quarter_booked_deals_df\n",
    "    inq_arr_by_bt_unstacked = in_quarter_booked_deals_df.groupby(\n",
    "        ['Bookings Type', 'Fiscal Period - Corrected'], observed=False \n",
    "    )['ARR Change'].sum().unstack(fill_value=0)\n",
    "\n",
    "    # Calculate Total Bookings based ONLY on all Closed Won deals\n",
    "    total_arr_by_fp_series = booked_deals_df.groupby(\n",
    "        'Fiscal Period - Corrected', observed=False\n",
    "    )['ARR Change'].sum()\n",
    "\n",
    "    summary_by_bookings_type_final = pd.DataFrame() \n",
    "    if not inq_arr_by_bt_unstacked.empty or not total_arr_by_fp_series.empty: # Proceed if either has data\n",
    "        # If in-quarter is empty but total is not, start with an empty DF with the right columns\n",
    "        if inq_arr_by_bt_unstacked.empty:\n",
    "            all_btypes = booked_deals_df['Bookings Type'].dropna().unique()\n",
    "            all_quarters = booked_deals_df['Fiscal Period - Corrected'].dropna().unique()\n",
    "            summary_by_bookings_type_final = pd.DataFrame(0, index=all_btypes, columns=all_quarters)\n",
    "        else:\n",
    "            summary_by_bookings_type_final = inq_arr_by_bt_unstacked.copy()\n",
    "\n",
    "        summary_by_bookings_type_final.loc['Total Inquarter'] = summary_by_bookings_type_final.sum(axis=0)\n",
    "        \n",
    "        if not total_arr_by_fp_series.empty:\n",
    "            summary_by_bookings_type_final.loc['Total bookings'] = total_arr_by_fp_series.reindex(summary_by_bookings_type_final.columns).fillna(0)\n",
    "        else:\n",
    "            summary_by_bookings_type_final.loc['Total bookings'] = 0 \n",
    "\n",
    "        if 'Total Inquarter' in summary_by_bookings_type_final.index and 'Total bookings' in summary_by_bookings_type_final.index:\n",
    "            percent_inquarter_bt_series = (summary_by_bookings_type_final.loc['Total Inquarter'] / summary_by_bookings_type_final.loc['Total bookings'].replace(0, np.nan) * 100).fillna(0)\n",
    "            summary_by_bookings_type_final.loc['Percent inquarter'] = percent_inquarter_bt_series.apply(lambda x: f\"{x:.2f}%\" if pd.notnull(x) else \"0.00%\")\n",
    "        else:\n",
    "            summary_by_bookings_type_final.loc['Percent inquarter'] = \"N/A\"\n",
    "\n",
    "    fy_start_str_display = f\"FY{START_FISCAL_YEAR_FOR_ANALYSIS}Q1\"\n",
    "    \n",
    "    if not summary_by_bookings_type_final.empty:\n",
    "        cols_to_keep_bt = [col for col in summary_by_bookings_type_final.columns if isinstance(col, str) and col >= fy_start_str_display]\n",
    "        summary_by_bookings_type_filtered = summary_by_bookings_type_final[sorted(cols_to_keep_bt)]\n",
    "        \n",
    "        # Add comparison columns for numeric rows only\n",
    "        numeric_rows = summary_by_bookings_type_filtered.index[summary_by_bookings_type_filtered.index != 'Percent inquarter']\n",
    "        summary_numeric = summary_by_bookings_type_filtered.loc[numeric_rows].copy()\n",
    "        summary_numeric = add_comparison_columns(summary_numeric)\n",
    "        \n",
    "        # Recombine with percent row\n",
    "        if 'Percent inquarter' in summary_by_bookings_type_filtered.index:\n",
    "            percent_row = summary_by_bookings_type_filtered.loc[['Percent inquarter']].copy()\n",
    "            # Add empty comparison columns to percent row\n",
    "            for col in ['QoQ Change', 'vs Last 4Q Avg', 'YoY Change']:\n",
    "                percent_row[col] = 'N/A'\n",
    "            summary_by_bookings_type_filtered = pd.concat([summary_numeric, percent_row])\n",
    "        else:\n",
    "            summary_by_bookings_type_filtered = summary_numeric\n",
    "        \n",
    "        print(f\"\\nDisplaying: Summary by Bookings Type (Filtered to display from {fy_start_str_display})\")\n",
    "        # 2. --- CHANGED --- Use display() instead of print() for a rich, scrollable HTML table\n",
    "        display(summary_by_bookings_type_filtered)\n",
    "    else:\n",
    "        print(\"Could not generate 'Summary by Bookings Type'.\")\n",
    "        \n",
    "    summary_by_segment_bookings_type_final = pd.DataFrame()\n",
    "    if 'Segment - historical' in processed_df.columns:\n",
    "        print(f\"\\n\\n{'='*15} Generating Main Summary Table: By Segment & Bookings Type (ARR Focused) {'='*15}\")\n",
    "        \n",
    "        inq_arr_by_seg_bt_unstacked = in_quarter_booked_deals_df.groupby(\n",
    "            ['Segment - historical', 'Bookings Type', 'Fiscal Period - Corrected'], observed=False\n",
    "        )['ARR Change'].sum().unstack(fill_value=0)\n",
    "        \n",
    "        total_arr_by_seg_fp_unstacked = booked_deals_df.groupby(\n",
    "            ['Segment - historical', 'Fiscal Period - Corrected'], observed=False\n",
    "        )['ARR Change'].sum().unstack(fill_value=0)\n",
    "        \n",
    "        all_segments_summary_list = []\n",
    "        if not total_arr_by_seg_fp_unstacked.empty: \n",
    "            sorted_fiscal_quarters_cols = sorted([q for q in processed_df['Fiscal Period - Corrected'].dropna().unique() if q is not None])\n",
    "            for segment_name in sorted(processed_df['Segment - historical'].dropna().unique()):\n",
    "                segment_summary_df = pd.DataFrame(columns=sorted_fiscal_quarters_cols) \n",
    "                if segment_name in inq_arr_by_seg_bt_unstacked.index.get_level_values('Segment - historical'):\n",
    "                    segment_inq_arr_for_bt_df = inq_arr_by_seg_bt_unstacked.loc[segment_name]\n",
    "                    if isinstance(segment_inq_arr_for_bt_df, pd.Series): \n",
    "                        segment_inq_arr_for_bt_df = segment_inq_arr_for_bt_df.to_frame().T \n",
    "                    segment_inq_arr_for_bt_df = segment_inq_arr_for_bt_df.reindex(columns=sorted_fiscal_quarters_cols, fill_value=0)\n",
    "                    segment_summary_df = pd.concat([segment_summary_df, segment_inq_arr_for_bt_df])\n",
    "                total_inq_for_segment_series = segment_summary_df.sum(axis=0) \n",
    "                segment_summary_df.loc['Total Inquarter'] = total_inq_for_segment_series\n",
    "                if segment_name in total_arr_by_seg_fp_unstacked.index:\n",
    "                    total_bookings_for_segment_series = total_arr_by_seg_fp_unstacked.loc[segment_name].reindex(\n",
    "                        index=sorted_fiscal_quarters_cols, fill_value=0 \n",
    "                    )\n",
    "                    segment_summary_df.loc['Total bookings'] = total_bookings_for_segment_series\n",
    "                else: \n",
    "                    segment_summary_df.loc['Total bookings'] = 0 \n",
    "                if 'Total Inquarter' in segment_summary_df.index and 'Total bookings' in segment_summary_df.index:\n",
    "                    percent_inq_for_segment_series = (segment_summary_df.loc['Total Inquarter'] / segment_summary_df.loc['Total bookings'].replace(0, np.nan) * 100).fillna(0)\n",
    "                    segment_summary_df.loc['Percent inquarter'] = percent_inq_for_segment_series.apply(lambda x: f\"{x:.2f}%\" if pd.notnull(x) else \"0.00%\")\n",
    "                else:\n",
    "                    segment_summary_df.loc['Percent inquarter'] = \"N/A\"\n",
    "                segment_summary_df.index = pd.MultiIndex.from_product([[segment_name], segment_summary_df.index], names=['Segment - historical', 'Bookings Type / Metric'])\n",
    "                all_segments_summary_list.append(segment_summary_df)\n",
    "            if all_segments_summary_list:\n",
    "                summary_by_segment_bookings_type_final = pd.concat(all_segments_summary_list)\n",
    "\n",
    "    if not summary_by_segment_bookings_type_final.empty:\n",
    "        cols_to_keep_seg = [col for col in summary_by_segment_bookings_type_final.columns if isinstance(col, str) and col >= fy_start_str_display]\n",
    "        summary_by_segment_bt_filtered = summary_by_segment_bookings_type_final[sorted(cols_to_keep_seg)]\n",
    "        \n",
    "        # Add comparison columns for each segment separately\n",
    "        segments = summary_by_segment_bt_filtered.index.get_level_values(0).unique()\n",
    "        segment_dataframes = []\n",
    "        \n",
    "        for segment in segments:\n",
    "            segment_data = summary_by_segment_bt_filtered.loc[segment].copy()\n",
    "            # Identify numeric rows (exclude Percent inquarter)\n",
    "            numeric_rows = segment_data.index[segment_data.index != 'Percent inquarter']\n",
    "            if len(numeric_rows) > 0:\n",
    "                segment_numeric = segment_data.loc[numeric_rows].copy()\n",
    "                segment_numeric = add_comparison_columns(segment_numeric)\n",
    "                \n",
    "                # Add back percent row if it exists\n",
    "                if 'Percent inquarter' in segment_data.index:\n",
    "                    percent_row = segment_data.loc[['Percent inquarter']].copy()\n",
    "                    # Add empty comparison columns to percent row\n",
    "                    for col in ['QoQ Change', 'vs Last 4Q Avg', 'YoY Change']:\n",
    "                        percent_row[col] = 'N/A'\n",
    "                    segment_data = pd.concat([segment_numeric, percent_row])\n",
    "                else:\n",
    "                    segment_data = segment_numeric\n",
    "            \n",
    "            # Restore the multi-index\n",
    "            segment_data.index = pd.MultiIndex.from_product([[segment], segment_data.index], names=['Segment - historical', 'Bookings Type / Metric'])\n",
    "            segment_dataframes.append(segment_data)\n",
    "        \n",
    "        if segment_dataframes:\n",
    "            summary_by_segment_bt_filtered = pd.concat(segment_dataframes)\n",
    "        \n",
    "        print(f\"\\nDisplaying: Summary by Segment & Bookings Type (Filtered to display from {fy_start_str_display})\")\n",
    "        # 3. --- CHANGED --- Use display() here as well for the second table\n",
    "        display(summary_by_segment_bt_filtered)\n",
    "    else:\n",
    "        print(\"Could not generate or display 'Summary by Segment & Bookings Type'.\")\n",
    "else:\n",
    "    print(\"Main summary table generation skipped: `processed_df` is empty or critical columns for summary are missing.\")\n",
    "\n",
    "print(\"--- Finished Attempting to Generate Main Summary Tables ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b1a20d-7ae4-4220-95f2-8a01321beaf4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-03T00:02:45.929199Z",
     "iopub.status.idle": "2025-07-03T00:02:45.929363Z",
     "shell.execute_reply": "2025-07-03T00:02:45.929294Z",
     "shell.execute_reply.started": "2025-07-03T00:02:45.929286Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Comprehensive Trend Analysis: Complete Data Readout (No Threshold Filtering)\n",
    "# \n",
    "# This section performs a comprehensive trend analysis and captures ALL comparisons.\n",
    "# It compares each quarter against three different benchmarks:\n",
    "# 1.  **Quarter-over-Quarter (QoQ):** Performance vs. the immediately preceding quarter.\n",
    "# 2.  **Year-over-Year (YoY):** Performance vs. the same quarter in the prior fiscal year.\n",
    "# 3.  **vs. 4-Quarter Average:** Performance vs. the average of the four preceding quarters.\n",
    "#\n",
    "# Unlike the threshold-based version, this captures EVERY comparison for user self-service analysis.\n",
    "\n",
    "def clean_group_label(group_label):\n",
    "    \"\"\"\n",
    "    Clean up group labels by removing verbose prefixes.\n",
    "    \"\"\"\n",
    "    # Remove common prefixes\n",
    "    prefixes_to_remove = [\n",
    "        \"By Segment & Bookings Type: \",\n",
    "        \"By Bookings Type: \",\n",
    "        \"By Segment: \",\n",
    "        \"Overall Inquarter\",\n",
    "        \"Overall Total\"\n",
    "    ]\n",
    "    \n",
    "    cleaned_label = group_label\n",
    "    for prefix in prefixes_to_remove:\n",
    "        if cleaned_label.startswith(prefix):\n",
    "            cleaned_label = cleaned_label[len(prefix):]\n",
    "            break\n",
    "    \n",
    "    # Handle the \"Overall\" cases\n",
    "    if group_label == \"Overall Inquarter\":\n",
    "        cleaned_label = \"Overall Inquarter\"\n",
    "    elif group_label == \"Overall Total\":\n",
    "        cleaned_label = \"Overall Total\"\n",
    "    \n",
    "    return cleaned_label\n",
    "\n",
    "def record_all_changes(comparison_df, current_q, comparison_label, group_label=\"Overall\"):\n",
    "    \"\"\"\n",
    "    Records ALL metric changes without any threshold filtering.\n",
    "    Returns a list of all comparisons for analysis.\n",
    "    \"\"\"\n",
    "    all_changes_list = []\n",
    "    if comparison_df.empty: \n",
    "        return all_changes_list\n",
    "    \n",
    "    METRICS_TO_ANALYZE = ['Total ARR', '# of Opps', '# of Won Opps', 'Won ARR', 'Avg Sales Price', 'Win Rate (Count)', 'Avg Sales Cycle']\n",
    "    \n",
    "    for metric_name in METRICS_TO_ANALYZE:\n",
    "        if metric_name not in comparison_df.index: \n",
    "            continue\n",
    "            \n",
    "        row_data = comparison_df.loc[metric_name]\n",
    "        current_val = row_data.get('Current Quarter', 0)\n",
    "        comparison_val = row_data.get(comparison_label, 0)\n",
    "        pct_change = row_data.get('Percent Change', 0)\n",
    "        abs_change = row_data.get('Absolute Change', 0)\n",
    "        \n",
    "        # Convert to numeric, handling any non-numeric values\n",
    "        try:\n",
    "            current_val = float(current_val) if pd.notnull(current_val) else 0\n",
    "            comparison_val = float(comparison_val) if pd.notnull(comparison_val) else 0\n",
    "            pct_change = float(pct_change) if pd.notnull(pct_change) else 0\n",
    "            abs_change = float(abs_change) if pd.notnull(abs_change) else 0\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "            \n",
    "        all_changes_list.append({\n",
    "            'Quarter': current_q,\n",
    "            'Comparison_Type': comparison_label,\n",
    "            'Group': clean_group_label(group_label),\n",
    "            'Metric': metric_name,\n",
    "            'Current_Value': current_val,\n",
    "            'Comparison_Value': comparison_val,\n",
    "            'Absolute_Change': abs_change,\n",
    "            'Percent_Change': pct_change,\n",
    "            'Direction': 'Increase' if abs_change > 0 else ('Decrease' if abs_change < 0 else 'No Change')\n",
    "        })\n",
    "    \n",
    "    return all_changes_list\n",
    "\n",
    "# Initialize the complete trends list\n",
    "all_trends_complete = []\n",
    "\n",
    "processed_df_for_trends = pd.DataFrame()\n",
    "inq_df_for_trends = pd.DataFrame()\n",
    "fy_start_str_trends = f\"FY{START_FISCAL_YEAR_FOR_ANALYSIS}Q1\"\n",
    "\n",
    "if not processed_df.empty and 'Fiscal Period - Corrected' in processed_df.columns:\n",
    "    processed_df_for_trends = processed_df[processed_df['Fiscal Period - Corrected'] >= fy_start_str_trends].copy()\n",
    "    if not inq_df.empty:\n",
    "        inq_df_for_trends = inq_df[inq_df['Fiscal Period - Corrected'] >= fy_start_str_trends].copy()\n",
    "\n",
    "if not processed_df_for_trends.empty and OPP_ID_COL_NAME and OPP_ID_COL_NAME in processed_df_for_trends.columns:\n",
    "    \n",
    "    grouping_definitions = {\n",
    "        \"Overall Inquarter\": {'df': inq_df_for_trends, 'cols': None},\n",
    "        \"Overall Total\": {'df': processed_df_for_trends, 'cols': None},\n",
    "        \"By Bookings Type\": {'df': processed_df_for_trends, 'cols': 'Bookings Type'},\n",
    "        \"By Segment\": {'df': processed_df_for_trends, 'cols': 'Segment - historical'},\n",
    "        \"By Segment & Bookings Type\": {'df': processed_df_for_trends, 'cols': ['Segment - historical', 'Bookings Type']}\n",
    "    }\n",
    "    \n",
    "    for group_name, G in grouping_definitions.items():\n",
    "        cols_to_check = G['cols']\n",
    "        if isinstance(cols_to_check, str): cols_to_check = [cols_to_check]\n",
    "        \n",
    "        if G['df'].empty or (cols_to_check and any(c not in G['df'].columns for c in cols_to_check)):\n",
    "            print(f\"Skipping grouping '{group_name}' due to missing data or columns.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing trends for: {group_name}\")\n",
    "        \n",
    "        all_q_metrics = calculate_quarterly_metrics(G['df'], OPP_ID_COL_NAME, group_by_cols=G['cols'])\n",
    "        if all_q_metrics.empty:\n",
    "            print(f\"Could not calculate quarterly metrics for {group_name}.\")\n",
    "            continue\n",
    "            \n",
    "        quarters_in_data = sorted([q for q in all_q_metrics.index.get_level_values('Fiscal Period - Corrected').unique() if q is not None])\n",
    "        \n",
    "        for i, current_q in enumerate(quarters_in_data):\n",
    "            current_q_groups = all_q_metrics[all_q_metrics.index.get_level_values('Fiscal Period - Corrected') == current_q]\n",
    "            if current_q_groups.empty: continue\n",
    "            \n",
    "            # --- Comparison 1: QoQ ---\n",
    "            if i > 0:\n",
    "                prev_q = quarters_in_data[i-1]\n",
    "                comparison_label = f\"QoQ vs {prev_q}\"\n",
    "                if G['cols']: \n",
    "                    for group_tuple in current_q_groups.index.droplevel('Fiscal Period - Corrected'):\n",
    "                        group_as_tuple = group_tuple if isinstance(group_tuple, tuple) else (group_tuple,)\n",
    "                        idx_key = group_as_tuple + (prev_q,)\n",
    "                        if idx_key in all_q_metrics.index:\n",
    "                             comparison_table = build_comparison_view(all_q_metrics, current_q, all_q_metrics.loc[idx_key], comparison_label, group_tuple=group_tuple)\n",
    "                             group_label_str = \" - \".join(map(str, group_as_tuple))\n",
    "                             all_trends_complete.extend(record_all_changes(comparison_table, current_q, comparison_label, group_label_str))\n",
    "                else: # Not grouped\n",
    "                    if prev_q in all_q_metrics.index:\n",
    "                        comparison_table = build_comparison_view(all_q_metrics, current_q, all_q_metrics.loc[prev_q], comparison_label)\n",
    "                        all_trends_complete.extend(record_all_changes(comparison_table, current_q, comparison_label, group_name))\n",
    "\n",
    "            # --- Comparison 2: YoY ---\n",
    "            year, q_num = int(current_q[2:6]), int(current_q[-1])\n",
    "            prev_year_q = f\"FY{year-1}Q{q_num}\"\n",
    "            if prev_year_q in quarters_in_data:\n",
    "                comparison_label = f\"YoY vs {prev_year_q}\"\n",
    "                if G['cols']:\n",
    "                    for group_tuple in current_q_groups.index.droplevel('Fiscal Period - Corrected'):\n",
    "                        group_as_tuple = group_tuple if isinstance(group_tuple, tuple) else (group_tuple,)\n",
    "                        idx_key = group_as_tuple + (prev_year_q,)\n",
    "                        if idx_key in all_q_metrics.index:\n",
    "                            comparison_table = build_comparison_view(all_q_metrics, current_q, all_q_metrics.loc[idx_key], comparison_label, group_tuple=group_tuple)\n",
    "                            group_label_str = \" - \".join(map(str, group_as_tuple))\n",
    "                            all_trends_complete.extend(record_all_changes(comparison_table, current_q, comparison_label, group_label_str))\n",
    "                else: # Not grouped\n",
    "                    if prev_year_q in all_q_metrics.index:\n",
    "                        comparison_table = build_comparison_view(all_q_metrics, current_q, all_q_metrics.loc[prev_year_q], comparison_label)\n",
    "                        all_trends_complete.extend(record_all_changes(comparison_table, current_q, comparison_label, group_name))\n",
    "\n",
    "            # --- Comparison 3: vs. Prior 4-Quarter Average ---\n",
    "            if i >= 4:\n",
    "                prior_4_quarters = quarters_in_data[i-4:i]\n",
    "                comparison_label = f\"vs 4Q Avg ({prior_4_quarters[0]}-{prior_4_quarters[-1]})\"\n",
    "                if G['cols']:\n",
    "                    for group_tuple in current_q_groups.index.droplevel('Fiscal Period - Corrected'):\n",
    "                        try:\n",
    "                            # Select all data for the specific group, then filter for the prior 4 quarters\n",
    "                            data_for_group = all_q_metrics.loc[group_tuple]\n",
    "                            prior_4q_group_data = data_for_group.loc[data_for_group.index.isin(prior_4_quarters)]\n",
    "                            \n",
    "                            # Only proceed if we have data for all 4 prior quarters for this specific group\n",
    "                            if len(prior_4q_group_data) == 4:\n",
    "                                prior_4q_avg_group = prior_4q_group_data.mean()\n",
    "                                comparison_table = build_comparison_view(all_q_metrics, current_q, prior_4q_avg_group, comparison_label, group_tuple=group_tuple)\n",
    "                                group_as_tuple = group_tuple if isinstance(group_tuple, tuple) else (group_tuple,)\n",
    "                                group_label_str = \" - \".join(map(str, group_as_tuple))\n",
    "                                all_trends_complete.extend(record_all_changes(comparison_table, current_q, comparison_label, group_label_str))\n",
    "                        except (KeyError, IndexError):\n",
    "                            continue\n",
    "                else: # Not grouped\n",
    "                    prior_4q_avg = all_q_metrics.loc[prior_4_quarters].mean()\n",
    "                    comparison_table = build_comparison_view(all_q_metrics, current_q, prior_4q_avg, comparison_label)\n",
    "                    all_trends_complete.extend(record_all_changes(comparison_table, current_q, comparison_label, group_name))\n",
    "\n",
    "    # Create the complete trends DataFrame\n",
    "    if all_trends_complete:\n",
    "        print(f\"\\n{'='*25} COMPLETE TRENDS ANALYSIS READOUT {'='*25}\")\n",
    "        complete_trends_df = pd.DataFrame(all_trends_complete)\n",
    "        \n",
    "        # Reorder columns for better readability\n",
    "        column_order = ['Quarter', 'Comparison_Type', 'Group', 'Metric', 'Current_Value', 'Comparison_Value', \n",
    "                       'Absolute_Change', 'Percent_Change', 'Direction']\n",
    "        complete_trends_df = complete_trends_df[column_order]\n",
    "        \n",
    "        # Sort for logical viewing\n",
    "        complete_trends_df = complete_trends_df.sort_values(by=['Quarter', 'Group', 'Comparison_Type', 'Metric'])\n",
    "        \n",
    "        # Display summary stats\n",
    "        print(f\"Total trend comparisons captured: {len(complete_trends_df)}\")\n",
    "        print(f\"Quarters analyzed: {complete_trends_df['Quarter'].nunique()}\")\n",
    "        print(f\"Groups analyzed: {complete_trends_df['Group'].nunique()}\")\n",
    "        print(f\"Metrics tracked: {complete_trends_df['Metric'].nunique()}\")\n",
    "        \n",
    "        # Show a sample of the data\n",
    "        print(f\"\\nSample of complete trends data (first 10 rows):\")\n",
    "        display(complete_trends_df.head(10))\n",
    "        \n",
    "        # Save to CSV for user analysis\n",
    "        complete_trends_df.to_csv('complete_trends_analysis.csv', index=False)\n",
    "        print(f\"\\nComplete trends analysis saved to 'complete_trends_analysis.csv'\")\n",
    "        print(\"Users can now filter and analyze this data based on their own criteria.\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n\\nNo trend data could be generated.\")\n",
    "else:\n",
    "    print(\"Complete trends analysis skipped: Processed DataFrame is empty or critical columns are missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45kx8ck5grh",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-03T00:02:45.930123Z",
     "iopub.status.idle": "2025-07-03T00:02:45.930387Z",
     "shell.execute_reply": "2025-07-03T00:02:45.930231Z",
     "shell.execute_reply.started": "2025-07-03T00:02:45.930207Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Win Rate Analysis - 6 Row Structure (Using Module)\n",
    "#\n",
    "# This section performs 6-row win rate analysis using the modular approach.\n",
    "\n",
    "from metrics_calculator import calculate_win_rate_analysis\n",
    "\n",
    "print(\"🎯 WIN RATE 6-ROW ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check if processed_df exists and has required data\n",
    "if 'processed_df' in locals() and not processed_df.empty and 'OPP_ID_COL_NAME' in locals():\n",
    "    # Calculate win rate analysis\n",
    "    win_rate_result = calculate_win_rate_analysis(\n",
    "        df=processed_df,\n",
    "        opp_id_col=OPP_ID_COL_NAME,\n",
    "        stage_col='Stage',\n",
    "        segment_col='Segment - historical',\n",
    "        quarter_col='Fiscal Period - Corrected'\n",
    "    )\n",
    "    \n",
    "    if not win_rate_result.empty:\n",
    "        print(\"\\n✅ Win Rate 6-row analysis completed successfully!\")\n",
    "        print(\"\\n📊 Win Rate Analysis Results:\")\n",
    "        for index, row in win_rate_result.iterrows():\n",
    "            print(f\"{row['Metric']:<25}: {row['Win Rate (%)']:>8.1f}%\")\n",
    "        \n",
    "        print(\"\\n📋 Full Results DataFrame:\")\n",
    "        display(win_rate_result)\n",
    "    else:\n",
    "        print(\"❌ Could not generate win rate analysis results\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Required data not found. Please ensure:\")\n",
    "    print(\"   1. processed_df exists (run Data Preprocessing cell)\")\n",
    "    print(\"   2. OPP_ID_COL_NAME is defined\")\n",
    "    print(\"   3. Required columns exist in processed_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amkfqh05sq",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-03T00:02:45.931278Z",
     "iopub.status.idle": "2025-07-03T00:02:45.931624Z",
     "shell.execute_reply": "2025-07-03T00:02:45.931549Z",
     "shell.execute_reply.started": "2025-07-03T00:02:45.931543Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## ASP (Average Sales Price) Analysis - 6 Row Structure (Using Module)\n",
    "#\n",
    "# This section performs 6-row Average Sales Price analysis using the modular approach.\n",
    "\n",
    "from metrics_calculator import calculate_asp_analysis\n",
    "\n",
    "print(\"🎯 AVERAGE SALES PRICE 6-ROW ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Check if processed_df exists and has required data\n",
    "if 'processed_df' in locals() and not processed_df.empty and 'OPP_ID_COL_NAME' in locals():\n",
    "    # Calculate ASP analysis\n",
    "    asp_result = calculate_asp_analysis(\n",
    "        df=processed_df,\n",
    "        arr_col='ARR Change',\n",
    "        opp_id_col=OPP_ID_COL_NAME,\n",
    "        stage_col='Stage',\n",
    "        segment_col='Segment - historical',\n",
    "        quarter_col='Fiscal Period - Corrected'\n",
    "    )\n",
    "    \n",
    "    if not asp_result.empty:\n",
    "        print(\"\\n✅ ASP 6-row analysis completed successfully!\")\n",
    "        print(\"\\n📊 Average Sales Price Analysis Results:\")\n",
    "        for index, row in asp_result.iterrows():\n",
    "            print(f\"{row['Metric']:<25}: ${row['Avg Sales Price ($)']:>12,.0f}\")\n",
    "        \n",
    "        print(\"\\n📋 Full Results DataFrame:\")\n",
    "        display(asp_result)\n",
    "    else:\n",
    "        print(\"❌ Could not generate ASP analysis results\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Required data not found. Please ensure:\")\n",
    "    print(\"   1. processed_df exists (run Data Preprocessing cell)\")\n",
    "    print(\"   2. OPP_ID_COL_NAME is defined\")\n",
    "    print(\"   3. Required columns exist in processed_df\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "8dq5gab4ezt",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-03T00:02:45.932255Z",
     "iopub.status.idle": "2025-07-03T00:02:45.932409Z",
     "shell.execute_reply": "2025-07-03T00:02:45.932306Z",
     "shell.execute_reply.started": "2025-07-03T00:02:45.932300Z"
    }
   },
   "outputs": [],
   "source": "# ## ASC (Average Sales Cycle) Analysis - 6 Row Structure (Using Module)\n#\n# This section performs 6-row Average Sales Cycle analysis using the modular approach.\n\nfrom metrics_calculator import calculate_asc_analysis\n\nprint(\"🎯 AVERAGE SALES CYCLE 6-ROW ANALYSIS\")\nprint(\"=\" * 45)\n\n# Check if processed_df exists and has required data\nif 'processed_df' in locals() and not processed_df.empty and 'OPP_ID_COL_NAME' in locals():\n    # Calculate ASC analysis - returns two DataFrames (simple and multi-index versions)\n    asc_simple, asc_multi_index = calculate_asc_analysis(\n        df=processed_df,\n        opp_id_col=OPP_ID_COL_NAME,\n        stage_col='Stage',\n        segment_col='Bookings Type',\n        quarter_col='Fiscal Period - Corrected',\n        created_col='Created Date',\n        close_col='Close Date'\n    )\n    \n    if not asc_simple.empty:\n        print(\"\\n✅ ASC 6-row analysis completed successfully!\")\n        \n        print(\"\\n📊 Simple ASC Analysis (Bookings Type View):\")\n        display(asc_simple)\n        \n        if not asc_multi_index.empty:\n            print(\"\\n📊 Detailed ASC Analysis (Segment & Bookings Type View):\")\n            display(asc_multi_index)\n        else:\n            print(\"\\n⚠️ Multi-index ASC analysis empty - no segment breakdown available\")\n            \n    else:\n        print(\"❌ Could not generate ASC analysis results\")\n        \nelse:\n    print(\"❌ Required data not found. Please ensure:\")\n    print(\"   1. processed_df exists (run Data Preprocessing cell)\")\n    print(\"   2. OPP_ID_COL_NAME is defined\")\n    print(\"   3. Required columns exist in processed_df\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rygg4i65wos",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-03T00:02:45.933362Z",
     "iopub.status.idle": "2025-07-03T00:02:45.933499Z",
     "shell.execute_reply": "2025-07-03T00:02:45.933419Z",
     "shell.execute_reply.started": "2025-07-03T00:02:45.933414Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Comprehensive Trend Analysis: Comparing QoQ, YoY, and vs. 4-Quarter Average\n",
    "# \n",
    "# This section performs a comprehensive trend analysis. It compares each quarter against three different benchmarks:\n",
    "# 1.  **Quarter-over-Quarter (QoQ):** Performance vs. the immediately preceding quarter.\n",
    "# 2.  **Year-over-Year (YoY):** Performance vs. the same quarter in the prior fiscal year.\n",
    "# 3.  **vs. 4-Quarter Average:** Performance vs. the average of the four preceding quarters.\n",
    "#\n",
    "# This is done for multiple data slices (Overall, By Segment, etc.) and all significant findings are collected into a single summary table.\n",
    "\n",
    "all_significant_changes_summary = []\n",
    "\n",
    "processed_df_for_trends = pd.DataFrame()\n",
    "inq_df_for_trends = pd.DataFrame()\n",
    "fy_start_str_trends = f\"FY{START_FISCAL_YEAR_FOR_ANALYSIS}Q1\"\n",
    "\n",
    "if not processed_df.empty and 'Fiscal Period - Corrected' in processed_df.columns:\n",
    "    processed_df_for_trends = processed_df[processed_df['Fiscal Period - Corrected'] >= fy_start_str_trends].copy()\n",
    "    if not inq_df.empty:\n",
    "        inq_df_for_trends = inq_df[inq_df['Fiscal Period - Corrected'] >= fy_start_str_trends].copy()\n",
    "\n",
    "if not processed_df_for_trends.empty and OPP_ID_COL_NAME and OPP_ID_COL_NAME in processed_df_for_trends.columns:\n",
    "    \n",
    "    grouping_definitions = {\n",
    "        \"Overall Inquarter\": {'df': inq_df_for_trends, 'cols': None},\n",
    "        \"Overall Total\": {'df': processed_df_for_trends, 'cols': None},\n",
    "        \"By Bookings Type\": {'df': processed_df_for_trends, 'cols': 'Bookings Type'},\n",
    "        \"By Segment\": {'df': processed_df_for_trends, 'cols': 'Segment - historical'},\n",
    "        \"By Segment & Bookings Type\": {'df': processed_df_for_trends, 'cols': ['Segment - historical', 'Bookings Type']}\n",
    "    }\n",
    "    \n",
    "    for group_name, G in grouping_definitions.items():\n",
    "        cols_to_check = G['cols']\n",
    "        if isinstance(cols_to_check, str): cols_to_check = [cols_to_check]\n",
    "        \n",
    "        if G['df'].empty or (cols_to_check and any(c not in G['df'].columns for c in cols_to_check)):\n",
    "            print(f\"Skipping grouping '{group_name}' due to missing data or columns.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n\\n{'='*25} Analyzing Trends for: {group_name} {'='*25}\")\n",
    "        \n",
    "        all_q_metrics = calculate_quarterly_metrics(G['df'], OPP_ID_COL_NAME, group_by_cols=G['cols'])\n",
    "        if all_q_metrics.empty:\n",
    "            print(f\"Could not calculate quarterly metrics for {group_name}.\")\n",
    "            continue\n",
    "            \n",
    "        quarters_in_data = sorted([q for q in all_q_metrics.index.get_level_values('Fiscal Period - Corrected').unique() if q is not None])\n",
    "        \n",
    "        for i, current_q in enumerate(quarters_in_data):\n",
    "            current_q_groups = all_q_metrics[all_q_metrics.index.get_level_values('Fiscal Period - Corrected') == current_q]\n",
    "            if current_q_groups.empty: continue\n",
    "            \n",
    "            # --- Comparison 1: QoQ ---\n",
    "            if i > 0:\n",
    "                prev_q = quarters_in_data[i-1]\n",
    "                comparison_label = f\"vs Prev Q ({prev_q})\"\n",
    "                if G['cols']: \n",
    "                    for group_tuple in current_q_groups.index.droplevel('Fiscal Period - Corrected'):\n",
    "                        group_as_tuple = group_tuple if isinstance(group_tuple, tuple) else (group_tuple,)\n",
    "                        idx_key = group_as_tuple + (prev_q,)\n",
    "                        if idx_key in all_q_metrics.index:\n",
    "                             comparison_table = build_comparison_view(all_q_metrics, current_q, all_q_metrics.loc[idx_key], comparison_label, group_tuple=group_tuple)\n",
    "                             group_label_str = \" - \".join(map(str, group_as_tuple))\n",
    "                             all_significant_changes_summary.extend(identify_and_report_significant_changes(comparison_table, current_q, comparison_label, f\"{group_name}: {group_label_str}\"))\n",
    "                else: # Not grouped\n",
    "                    if prev_q in all_q_metrics.index:\n",
    "                        comparison_table = build_comparison_view(all_q_metrics, current_q, all_q_metrics.loc[prev_q], comparison_label)\n",
    "                        all_significant_changes_summary.extend(identify_and_report_significant_changes(comparison_table, current_q, comparison_label, group_name))\n",
    "\n",
    "            # --- Comparison 2: YoY ---\n",
    "            year, q_num = int(current_q[2:6]), int(current_q[-1])\n",
    "            prev_year_q = f\"FY{year-1}Q{q_num}\"\n",
    "            if prev_year_q in quarters_in_data:\n",
    "                comparison_label = f\"vs Prev Year ({prev_year_q})\"\n",
    "                if G['cols']:\n",
    "                    for group_tuple in current_q_groups.index.droplevel('Fiscal Period - Corrected'):\n",
    "                        group_as_tuple = group_tuple if isinstance(group_tuple, tuple) else (group_tuple,)\n",
    "                        idx_key = group_as_tuple + (prev_year_q,)\n",
    "                        if idx_key in all_q_metrics.index:\n",
    "                            comparison_table = build_comparison_view(all_q_metrics, current_q, all_q_metrics.loc[idx_key], comparison_label, group_tuple=group_tuple)\n",
    "                            group_label_str = \" - \".join(map(str, group_as_tuple))\n",
    "                            all_significant_changes_summary.extend(identify_and_report_significant_changes(comparison_table, current_q, comparison_label, f\"{group_name}: {group_label_str}\"))\n",
    "                else: # Not grouped\n",
    "                    if prev_year_q in all_q_metrics.index:\n",
    "                        comparison_table = build_comparison_view(all_q_metrics, current_q, all_q_metrics.loc[prev_year_q], comparison_label)\n",
    "                        all_significant_changes_summary.extend(identify_and_report_significant_changes(comparison_table, current_q, comparison_label, group_name))\n",
    "\n",
    "            # --- Comparison 3: vs. Prior 4-Quarter Average ---\n",
    "            if i >= 4:\n",
    "                prior_4_quarters = quarters_in_data[i-4:i]\n",
    "                comparison_label = f\"vs Avg of {prior_4_quarters[0]}-{prior_4_quarters[-1]}\"\n",
    "                if G['cols']:\n",
    "                    for group_tuple in current_q_groups.index.droplevel('Fiscal Period - Corrected'):\n",
    "                        try:\n",
    "                            # Select all data for the specific group, then filter for the prior 4 quarters\n",
    "                            data_for_group = all_q_metrics.loc[group_tuple]\n",
    "                            prior_4q_group_data = data_for_group.loc[data_for_group.index.isin(prior_4_quarters)]\n",
    "                            \n",
    "                            # Only proceed if we have data for all 4 prior quarters for this specific group\n",
    "                            if len(prior_4q_group_data) == 4:\n",
    "                                prior_4q_avg_group = prior_4q_group_data.mean()\n",
    "                                comparison_table = build_comparison_view(all_q_metrics, current_q, prior_4q_avg_group, comparison_label, group_tuple=group_tuple)\n",
    "                                group_as_tuple = group_tuple if isinstance(group_tuple, tuple) else (group_tuple,)\n",
    "                                group_label_str = \" - \".join(map(str, group_as_tuple))\n",
    "                                all_significant_changes_summary.extend(identify_and_report_significant_changes(comparison_table, current_q, comparison_label, f\"{group_name}: {group_label_str}\"))\n",
    "                        except (KeyError, IndexError):\n",
    "                            continue\n",
    "                else: # Not grouped\n",
    "                    prior_4q_avg = all_q_metrics.loc[prior_4_quarters].mean()\n",
    "                    comparison_table = build_comparison_view(all_q_metrics, current_q, prior_4q_avg, comparison_label)\n",
    "                    all_significant_changes_summary.extend(identify_and_report_significant_changes(comparison_table, current_q, comparison_label, group_name))\n",
    "\n",
    "    if all_significant_changes_summary:\n",
    "        print(f\"\\n\\n{'='*25} SUMMARY OF ALL SIGNIFICANT TRENDS {'='*25}\")\n",
    "        significant_changes_summary_df = pd.DataFrame(all_significant_changes_summary)\n",
    "        summary_cols_order = ['Quarter', 'Comparison', 'Group', 'Metric', 'Type', '% Change', 'Abs Change']\n",
    "        print(significant_changes_summary_df[summary_cols_order].sort_values(by=['Quarter', 'Comparison', 'Group']))\n",
    "    else:\n",
    "        print(\"\\n\\nNo significant changes met the defined thresholds across all comparisons.\")\n",
    "else:\n",
    "    print(\"Significant changes summary skipped: Processed DataFrame is empty or critical columns are missing.\")\n",
    "\n",
    "if 'significant_changes_summary_df' in locals() and not significant_changes_summary_df.empty:\n",
    "    significant_changes_summary_df.to_csv('significant_trends_summary.csv', index=False)\n",
    "    print(\"\\nSuccessfully saved the significant trends summary to 'significant_trends_summary.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e3ce8-c5d7-4aaa-afcb-8af4d65cad99",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-03T00:02:45.934336Z",
     "iopub.status.idle": "2025-07-03T00:02:45.934531Z",
     "shell.execute_reply": "2025-07-03T00:02:45.934432Z",
     "shell.execute_reply.started": "2025-07-03T00:02:45.934427Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Prophet Forecasting for In-Quarter Bookings\n",
    "#\n",
    "# ### Data Preparation for Prophet Models\n",
    "\n",
    "prophet_data_overall = pd.DataFrame()\n",
    "prophet_data_by_segment = {}\n",
    "prophet_data_by_booking_type = {}\n",
    "\n",
    "inq_df_for_prophet = pd.DataFrame()\n",
    "fy_start_str_prophet = f\"FY{START_FISCAL_YEAR_FOR_ANALYSIS}Q1\"\n",
    "if not inq_df.empty and 'Fiscal Period - Corrected' in inq_df.columns:\n",
    "    inq_df_for_prophet = inq_df[inq_df['Fiscal Period - Corrected'] >= fy_start_str_prophet].copy()\n",
    "    print(f\"Data for Prophet models filtered to start from {fy_start_str_prophet}. Shape: {inq_df_for_prophet.shape}\")\n",
    "\n",
    "# Use 'ARR Change' for the 'y' value\n",
    "if not inq_df_for_prophet.empty and 'Close Date' in inq_df_for_prophet.columns and 'ARR Change' in inq_df_for_prophet.columns:\n",
    "    # Overall data\n",
    "    daily_inq_arr_overall_temp = inq_df_for_prophet.groupby(pd.Grouper(key='Close Date', freq='D'))['ARR Change'].sum().reset_index()\n",
    "    daily_inq_arr_overall_temp.rename(columns={'Close Date': 'ds', 'ARR Change': 'y'}, inplace=True)\n",
    "    prophet_data_overall = daily_inq_arr_overall_temp[daily_inq_arr_overall_temp['ds'].notna()].copy()\n",
    "    if not prophet_data_overall.empty: \n",
    "        print(f\"Overall daily in-quarter data prepared for Prophet. Shape: {prophet_data_overall.shape}\")\n",
    "    else: \n",
    "        print(\"No overall data for Prophet after preparation.\")\n",
    "\n",
    "    # Data by Segment - historical\n",
    "    if 'Segment - historical' in inq_df_for_prophet.columns:\n",
    "        for seg_name in inq_df_for_prophet['Segment - historical'].dropna().unique():\n",
    "            segment_data_temp = inq_df_for_prophet[inq_df_for_prophet['Segment - historical'] == seg_name]\n",
    "            daily_segment_arr_temp = segment_data_temp.groupby(pd.Grouper(key='Close Date', freq='D'))['ARR Change'].sum().reset_index()\n",
    "            daily_segment_arr_temp.rename(columns={'Close Date': 'ds', 'ARR Change': 'y'}, inplace=True)\n",
    "            prophet_data_by_segment[seg_name] = daily_segment_arr_temp[daily_segment_arr_temp['ds'].notna()].copy()\n",
    "            if not prophet_data_by_segment[seg_name].empty: \n",
    "                print(f\"Segment '{seg_name}' daily data prepared for Prophet. Shape: {prophet_data_by_segment[seg_name].shape}\")\n",
    "\n",
    "    # Data by Bookings Type\n",
    "    if 'Bookings Type' in inq_df_for_prophet.columns:\n",
    "        for bt_name in inq_df_for_prophet['Bookings Type'].dropna().unique():\n",
    "            booking_type_data_temp = inq_df_for_prophet[inq_df_for_prophet['Bookings Type'] == bt_name]\n",
    "            daily_bt_arr_temp = booking_type_data_temp.groupby(pd.Grouper(key='Close Date', freq='D'))['ARR Change'].sum().reset_index()\n",
    "            daily_bt_arr_temp.rename(columns={'Close Date': 'ds', 'ARR Change': 'y'}, inplace=True)\n",
    "            prophet_data_by_booking_type[bt_name] = daily_bt_arr_temp[daily_bt_arr_temp['ds'].notna()].copy()\n",
    "            if not prophet_data_by_booking_type[bt_name].empty: \n",
    "                print(f\"Booking Type '{bt_name}' daily data prepared for Prophet. Shape: {prophet_data_by_booking_type[bt_name].shape}\")\n",
    "else:\n",
    "    print(\"Prophet data preparation skipped: `inq_df_for_prophet` is empty or essential columns are missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6664e531-27fc-4921-bda3-0c4c77374d94",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-03T00:02:45.935035Z",
     "iopub.status.idle": "2025-07-03T00:02:45.935158Z",
     "shell.execute_reply": "2025-07-03T00:02:45.935090Z",
     "shell.execute_reply.started": "2025-07-03T00:02:45.935085Z"
    }
   },
   "outputs": [],
   "source": [
    "# ### Prophet Forecasting: Model Training & Prediction Function\n",
    "\n",
    "def run_prophet_forecast(time_series_df, series_descriptive_name=\"Series\", future_forecast_periods=92, \n",
    "                         seasonality_mode_prophet='additive', yearly_seasonality_prophet=True, \n",
    "                         weekly_seasonality_prophet=False, daily_seasonality_prophet=False):\n",
    "    \n",
    "    if time_series_df.empty or len(time_series_df) < 2: # Prophet needs at least 2 data points\n",
    "        print(f\"Prophet Warning: Not enough data for model for {series_descriptive_name}. Min 2 data points required, got {len(time_series_df)}.\")\n",
    "        return None, pd.DataFrame()\n",
    "\n",
    "    print(f\"\\nRunning Prophet for: {series_descriptive_name}\")\n",
    "    \n",
    "    prophet_model_instance = Prophet(\n",
    "        yearly_seasonality=yearly_seasonality_prophet,\n",
    "        weekly_seasonality=weekly_seasonality_prophet,\n",
    "        daily_seasonality=daily_seasonality_prophet,\n",
    "        seasonality_mode=seasonality_mode_prophet,\n",
    "        changepoint_prior_scale=0.05 # Default, adjust if over/underfitting trend\n",
    "    )\n",
    "    \n",
    "    # Optional: Add custom fiscal quarterly seasonality if yearly doesn't capture it well enough for your specific fiscal calendar\n",
    "    # This assumes a standard calendar year division for quarters. Adjust 'period' if your fiscal quarters have very specific lengths.\n",
    "    # fiscal_quarter_period_days = 365.25 / 4 \n",
    "    # prophet_model_instance.add_seasonality(name='fiscal_quarterly', period=fiscal_quarter_period_days, fourier_order=5) # fourier_order controls complexity\n",
    "\n",
    "    try:\n",
    "        prophet_model_instance.fit(time_series_df)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR fitting Prophet model for {series_descriptive_name}: {e}\")\n",
    "        print(\"Data sample (first 5 rows):\")\n",
    "        print(time_series_df.head())\n",
    "        return None, pd.DataFrame()\n",
    "\n",
    "    future_dates_df = prophet_model_instance.make_future_dataframe(periods=future_forecast_periods, freq='D')\n",
    "    forecast_results_df = prophet_model_instance.predict(future_dates_df)\n",
    "\n",
    "    print(f\"Prophet forecast for {series_descriptive_name} completed.\")\n",
    "\n",
    "    # Plotting Forecast\n",
    "    try:\n",
    "        fig_forecast = prophet_model_instance.plot(forecast_results_df)\n",
    "        plt.title(f'Forecast for {series_descriptive_name}', fontsize=16)\n",
    "        plt.xlabel('Date', fontsize=12)\n",
    "        plt.ylabel('Forecasted Value (yhat)', fontsize=12)\n",
    "        plt.show()\n",
    "    except Exception as e: print(f\"Error plotting forecast for {series_descriptive_name}: {e}\")\n",
    "\n",
    "    # Plotting Components\n",
    "    try:\n",
    "        fig_components = prophet_model_instance.plot_components(forecast_results_df)\n",
    "        fig_components.suptitle(f'Forecast Components for {series_descriptive_name}', fontsize=16, y=1.03) # Adjust y for title position\n",
    "        plt.show()\n",
    "    except Exception as e: print(f\"Error plotting components for {series_descriptive_name}: {e}\")\n",
    "\n",
    "    # Merge actuals 'y' back to forecast for easier comparison if needed\n",
    "    forecast_results_df_with_actuals = pd.merge(forecast_results_df, time_series_df[['ds', 'y']], on='ds', how='left')\n",
    "\n",
    "    return prophet_model_instance, forecast_results_df_with_actuals\n",
    "\n",
    "print(\"Cell 10: `run_prophet_forecast` function - Defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cf59c0-5aba-4794-b900-76afdcbadf38",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-03T00:02:45.935776Z",
     "iopub.status.idle": "2025-07-03T00:02:45.935931Z",
     "shell.execute_reply": "2025-07-03T00:02:45.935842Z",
     "shell.execute_reply.started": "2025-07-03T00:02:45.935836Z"
    }
   },
   "outputs": [],
   "source": [
    "# ### Prophet Forecasting: Execution & Storing Models/Forecasts\n",
    "\n",
    "DAYS_TO_FORECAST_PROPHET = 92 # Typical quarter length\n",
    "\n",
    "all_prophet_models = {}\n",
    "all_prophet_forecasts = {}\n",
    "\n",
    "# 1. Overall In-Quarter Bookings Forecast\n",
    "if not prophet_data_overall.empty:\n",
    "    model_overall_instance, forecast_overall_df = run_prophet_forecast(\n",
    "        prophet_data_overall,\n",
    "        series_descriptive_name=\"Overall In-Quarter Bookings\",\n",
    "        future_forecast_periods=DAYS_TO_FORECAST_PROPHET\n",
    "    )\n",
    "    if model_overall_instance: # Check if model training was successful\n",
    "        all_prophet_models['overall'] = model_overall_instance\n",
    "        all_prophet_forecasts['overall'] = forecast_overall_df\n",
    "        print(f\"--- Overall Forecast Details (tail) ---\")\n",
    "        print(forecast_overall_df[['ds', 'yhat', 'yhat_lower', 'yhat_upper', 'y']].tail())\n",
    "else:\n",
    "    print(\"Skipping Overall Prophet forecast as prepared data is empty.\")\n",
    "\n",
    "# 2. In-Quarter Bookings Forecast by Segment\n",
    "if prophet_data_by_segment: # Check if the dictionary itself is not empty\n",
    "    for segment_name_key, segment_timeseries_df in prophet_data_by_segment.items():\n",
    "        if not segment_timeseries_df.empty:\n",
    "            model_segment_instance, forecast_segment_df = run_prophet_forecast(\n",
    "                segment_timeseries_df,\n",
    "                series_descriptive_name=f\"Segment: {segment_name_key} In-Quarter Bookings\",\n",
    "                future_forecast_periods=DAYS_TO_FORECAST_PROPHET\n",
    "            )\n",
    "            if model_segment_instance:\n",
    "                all_prophet_models[f'segment_{segment_name_key}'] = model_segment_instance\n",
    "                all_prophet_forecasts[f'segment_{segment_name_key}'] = forecast_segment_df\n",
    "                print(f\"--- Forecast Details for Segment: {segment_name_key} (tail) ---\")\n",
    "                print(forecast_segment_df[['ds', 'yhat', 'yhat_lower', 'yhat_upper', 'y']].tail())\n",
    "        else:\n",
    "            print(f\"Skipping Prophet forecast for segment '{segment_name_key}' as its prepared data is empty.\")\n",
    "else:\n",
    "    print(\"Skipping Segment-level Prophet forecasts as no segment data was prepared.\")\n",
    "\n",
    "# 3. In-Quarter Bookings Forecast by Bookings Type\n",
    "if prophet_data_by_booking_type: # Check if the dictionary itself is not empty\n",
    "    for bt_name_key, bt_timeseries_df in prophet_data_by_booking_type.items():\n",
    "        if not bt_timeseries_df.empty:\n",
    "            model_bt_instance, forecast_bt_df = run_prophet_forecast(\n",
    "                bt_timeseries_df,\n",
    "                series_descriptive_name=f\"Bookings Type: {bt_name_key} In-Quarter Bookings\",\n",
    "                future_forecast_periods=DAYS_TO_FORECAST_PROPHET\n",
    "            )\n",
    "            if model_bt_instance:\n",
    "                all_prophet_models[f'booking_type_{bt_name_key}'] = model_bt_instance\n",
    "                all_prophet_forecasts[f'booking_type_{bt_name_key}'] = forecast_bt_df\n",
    "                print(f\"--- Forecast Details for Bookings Type: {bt_name_key} (tail) ---\")\n",
    "                print(forecast_bt_df[['ds', 'yhat', 'yhat_lower', 'yhat_upper', 'y']].tail())\n",
    "        else:\n",
    "            print(f\"Skipping Prophet forecast for bookings type '{bt_name_key}' as its prepared data is empty.\")\n",
    "else:\n",
    "    print(\"Skipping Booking Type-level Prophet forecasts as no booking type data was prepared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f8dcec-5655-46a8-af5c-522435ee8676",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-03T00:02:45.936766Z",
     "iopub.status.idle": "2025-07-03T00:02:45.936887Z",
     "shell.execute_reply": "2025-07-03T00:02:45.936815Z",
     "shell.execute_reply.started": "2025-07-03T00:02:45.936811Z"
    }
   },
   "outputs": [],
   "source": [
    "# ### Prophet Forecasting: Pacing Table Generation & \"Unseen\" Revenue Analysis\n",
    "\n",
    "# Determine the start date for \"future\" pacing tables\n",
    "# This would typically be the day after the last known data point in the overall series\n",
    "PACING_TABLE_START_DATE = None\n",
    "if not prophet_data_overall.empty and 'ds' in prophet_data_overall.columns and not prophet_data_overall['ds'].empty:\n",
    "    PACING_TABLE_START_DATE = prophet_data_overall['ds'].max() + pd.Timedelta(days=1)\n",
    "    print(f\"Pacing tables will generate forecasts starting from (day after last actual): {PACING_TABLE_START_DATE.strftime('%Y-%m-%d')}\")\n",
    "else:\n",
    "    print(\"Warning: Cannot determine PACING_TABLE_START_DATE from prophet_data_overall. Pacing tables might be impacted or skipped.\")\n",
    "\n",
    "def create_pacing_table_from_forecast(prophet_forecast_df, descriptive_name_series, future_pacing_start_date):\n",
    "    if prophet_forecast_df.empty:\n",
    "        print(f\"Cannot create pacing table for {descriptive_name_series}, input forecast_df is empty.\")\n",
    "        return pd.DataFrame()\n",
    "    if future_pacing_start_date is None:\n",
    "        print(f\"Cannot create pacing table for {descriptive_name_series}, future_pacing_start_date is None (likely no historical data to base it on).\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Filter for future dates based on the start date\n",
    "    pacing_df_filtered = prophet_forecast_df[prophet_forecast_df['ds'] >= pd.to_datetime(future_pacing_start_date)].copy()\n",
    "    \n",
    "    # Ensure required columns from Prophet output are present\n",
    "    required_prophet_cols = ['ds', 'yhat', 'yhat_lower', 'yhat_upper']\n",
    "    if not all(col in pacing_df_filtered.columns for col in required_prophet_cols):\n",
    "        print(f\"Error: Pacing table for {descriptive_name_series} - forecast DataFrame is missing one or more required Prophet columns: {required_prophet_cols}\")\n",
    "        print(\"Available columns in filtered forecast:\", pacing_df_filtered.columns.tolist())\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    pacing_df_selected = pacing_df_filtered[required_prophet_cols].copy() # Select only needed columns\n",
    "    \n",
    "    # Ensure yhat (forecasted value) and its bounds are not negative (e.g. ARR cannot be negative)\n",
    "    pacing_df_selected['yhat'] = np.maximum(0, pacing_df_selected['yhat'])\n",
    "    pacing_df_selected['yhat_lower'] = np.maximum(0, pacing_df_selected['yhat_lower'])\n",
    "    pacing_df_selected['yhat_upper'] = np.maximum(0, pacing_df_selected['yhat_upper'])\n",
    "    \n",
    "    # Calculate cumulative forecast using the (potentially floored) yhat\n",
    "    pacing_df_selected['Cumulative Forecast (yhat)'] = pacing_df_selected['yhat'].cumsum()\n",
    "    \n",
    "    # Rename columns for clarity in the final pacing table\n",
    "    pacing_df_selected.rename(columns={\n",
    "        'ds': 'Date', \n",
    "        'yhat': 'Forecasted Daily Value', # More generic than ARR if used for other metrics\n",
    "        'yhat_lower': 'Lower Bound Daily Value',\n",
    "        'yhat_upper': 'Upper Bound Daily Value'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    print(f\"\\n--- Pacing Table for {descriptive_name_series} (Next {DAYS_TO_FORECAST_PROPHET} Days) ---\")\n",
    "    if not pacing_df_selected.empty:\n",
    "        print(pacing_df_selected.head())\n",
    "        total_forecasted_for_period = pacing_df_selected['Forecasted Daily Value'].sum()\n",
    "        print(f\"Total Forecasted Value for {descriptive_name_series} over next {DAYS_TO_FORECAST_PROPHET} days: {total_forecasted_for_period:,.2f}\")\n",
    "    else:\n",
    "        print(\"Pacing table is empty (no future dates or other issue).\")\n",
    "\n",
    "    \n",
    "    # Plot cumulative forecast for the pacing period\n",
    "    if not pacing_df_selected.empty:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(pacing_df_selected['Date'], pacing_df_selected['Cumulative Forecast (yhat)'], \n",
    "                 label='Cumulative Forecast (yhat)', marker='.', linestyle='-')\n",
    "        \n",
    "        # Calculate cumulative sums for lower and upper bounds for the plot\n",
    "        cumulative_lower_plot = pacing_df_selected['Lower Bound Daily Value'].cumsum()\n",
    "        cumulative_upper_plot = pacing_df_selected['Upper Bound Daily Value'].cumsum()\n",
    "        \n",
    "        plt.fill_between(pacing_df_selected['Date'], \n",
    "                         cumulative_lower_plot, \n",
    "                         cumulative_upper_plot, \n",
    "                         alpha=0.2, label='Cumulative Confidence Interval')\n",
    "        \n",
    "        plt.title(f'Cumulative Forecasted Value - {descriptive_name_series}', fontsize=14)\n",
    "        plt.xlabel('Date', fontsize=12)\n",
    "        plt.ylabel('Cumulative Value', fontsize=12)\n",
    "        plt.legend()\n",
    "        plt.grid(True, which='both', linestyle=':')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return pacing_df_selected\n",
    "\n",
    "# Generate Pacing Tables\n",
    "if PACING_TABLE_START_DATE:\n",
    "    if 'overall' in all_prophet_forecasts and not all_prophet_forecasts['overall'].empty:\n",
    "        pacing_table_overall = create_pacing_table_from_forecast(\n",
    "            all_prophet_forecasts['overall'], \n",
    "            \"Overall In-Quarter Bookings\", \n",
    "            PACING_TABLE_START_DATE\n",
    "        )\n",
    "\n",
    "    for forecast_key, forecast_data_df in all_prophet_forecasts.items():\n",
    "        if forecast_key == 'overall': continue # Already handled\n",
    "\n",
    "        if not forecast_data_df.empty:\n",
    "            descriptive_name_for_pacing = forecast_key.replace('_', ' ').title() # e.g. \"Segment Enterprise\"\n",
    "            create_pacing_table_from_forecast(\n",
    "                forecast_data_df, \n",
    "                f\"{descriptive_name_for_pacing} In-Quarter Bookings\", \n",
    "                PACING_TABLE_START_DATE\n",
    "            )\n",
    "else:\n",
    "    print(\"Pacing table generation skipped as PACING_TABLE_START_DATE could not be determined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28c0b56-2f41-419f-bc04-b52215dc3fb3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-03T00:02:45.937415Z",
     "iopub.status.idle": "2025-07-03T00:02:45.937604Z",
     "shell.execute_reply": "2025-07-03T00:02:45.937530Z",
     "shell.execute_reply.started": "2025-07-03T00:02:45.937523Z"
    }
   },
   "outputs": [],
   "source": [
    "# **Discussion on \"Unseen\" Revenue:**\n",
    "# The `Forecasted Daily Value` (yhat) in the pacing tables represents Prophet's prediction for that day's in-quarter bookings. This prediction is based on historical patterns and inherently includes bookings expected from opportunities that are not yet created or visible in the current pipeline (i.e., \"unseen\" at the start of the forecast period but are expected to be created and closed within the quarter based on learned trends). The cumulative sum shows the total expected in-quarter bookings for the forecast horizon.\n",
    "\n",
    "# ### Prophet Forecasting: Seasonality Insights & Historical Quarterly Pacing Comparison\n",
    "\n",
    "def plot_historical_quarterly_pacing(daily_actuals_df_prophet_format, series_descriptive_name=\"Series\"):\n",
    "    if daily_actuals_df_prophet_format.empty or 'ds' not in daily_actuals_df_prophet_format.columns or 'y' not in daily_actuals_df_prophet_format.columns:\n",
    "        print(f\"Cannot plot historical pacing for {series_descriptive_name}, input DataFrame is empty or missing 'ds'/'y'.\")\n",
    "        return\n",
    "\n",
    "    plot_df_historical = daily_actuals_df_prophet_format.copy()\n",
    "    \n",
    "    # Apply get_fiscal_quarter_info and expand results into new columns\n",
    "    # We need: quarter_start_date (idx 0), quarter_label_str (idx 1)\n",
    "    fiscal_info_tuples = plot_df_historical['ds'].apply(lambda x: get_fiscal_quarter_info(x))\n",
    "    plot_df_historical['Quarter Start Date'] = fiscal_info_tuples.apply(lambda x: x[0])\n",
    "    plot_df_historical['Fiscal Quarter Label'] = fiscal_info_tuples.apply(lambda x: x[1])\n",
    "\n",
    "    # Drop rows where fiscal information couldn't be derived (e.g., NaT dates)\n",
    "    plot_df_historical.dropna(subset=['Fiscal Quarter Label', 'Quarter Start Date'], inplace=True)\n",
    "    if plot_df_historical.empty: \n",
    "        print(f\"No valid fiscal quarter data for {series_descriptive_name} after processing for historical pacing plot.\")\n",
    "        return\n",
    "\n",
    "    # Calculate Day Number within the Fiscal Quarter\n",
    "    plot_df_historical['Day of Quarter'] = (plot_df_historical['ds'] - plot_df_historical['Quarter Start Date']).dt.days + 1\n",
    "    \n",
    "    # Extract Quarter Type (Q1, Q2, Q3, Q4) for hue\n",
    "    plot_df_historical['Quarter Type'] = plot_df_historical['Fiscal Quarter Label'].str[-2:] # Takes 'Q1', 'Q2' etc.\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.lineplot(data=plot_df_historical, x='Day of Quarter', y='y', hue='Quarter Type', errorbar=('ci', 95), legend='full') # ci for confidence interval\n",
    "    plt.title(f'Historical Average Daily Value Pacing by Quarter Type - {series_descriptive_name}', fontsize=16)\n",
    "    plt.xlabel('Day of Fiscal Quarter', fontsize=12)\n",
    "    plt.ylabel('Average Daily Value (y)', fontsize=12)\n",
    "    plt.legend(title='Quarter Type', bbox_to_anchor=(1.05, 1), loc='upper left') # Move legend outside\n",
    "    plt.grid(True, which='both', linestyle=':', linewidth=0.7)\n",
    "    \n",
    "    max_day_observed = plot_df_historical['Day of Quarter'].max() if not plot_df_historical.empty else 95\n",
    "    plt.xlim(0, max_day_observed + 1 if pd.notnull(max_day_observed) else 95) # Adjust x-limit\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot historical quarterly pacing for overall data\n",
    "if not prophet_data_overall.empty:\n",
    "    plot_historical_quarterly_pacing(prophet_data_overall, \"Overall In-Quarter Bookings (Historical Data)\")\n",
    "else: \n",
    "    print(\"Skipping historical quarterly pacing plot for Overall data as prepared data is empty.\")\n",
    "\n",
    "# Plot for Segments\n",
    "if prophet_data_by_segment:\n",
    "    for segment_name, segment_df_for_prophet in prophet_data_by_segment.items():\n",
    "        if not segment_df_for_prophet.empty:\n",
    "            plot_historical_quarterly_pacing(segment_df_for_prophet, f\"Segment: {segment_name} (Historical Data)\")\n",
    "        else:\n",
    "            print(f\"Skipping historical quarterly pacing for segment '{segment_name}' as its prepared data is empty.\")\n",
    "\n",
    "# Plot for Bookings Types\n",
    "if prophet_data_by_booking_type:\n",
    "    for bt_name, bt_df_for_prophet in prophet_data_by_booking_type.items():\n",
    "        if not bt_df_for_prophet.empty:\n",
    "            plot_historical_quarterly_pacing(bt_df_for_prophet, f\"Bookings Type: {bt_name} (Historical Data)\")\n",
    "        else:\n",
    "            print(f\"Skipping historical quarterly pacing for bookings type '{bt_name}' as its prepared data is empty.\")\n",
    "\n",
    "print(\"\\nCell 13: Prophet Seasonality & Historical Pacing Plots - Executed (if data was available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb61716-beff-4d5d-97f5-c055ecfc62f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q4jgjnnowtq",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-03T00:02:45.938219Z",
     "iopub.status.idle": "2025-07-03T00:02:45.938346Z",
     "shell.execute_reply": "2025-07-03T00:02:45.938272Z",
     "shell.execute_reply.started": "2025-07-03T00:02:45.938267Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## QUICK TEST: Verify Modular Functions Work\n",
    "#\n",
    "# This cell tests if the .py modules can be imported and work correctly\n",
    "\n",
    "print(\"🔍 TESTING MODULAR FUNCTIONS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    from date_utils import enhance_dataframe_dates\n",
    "    print(\"✅ date_utils.py imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error importing date_utils: {e}\")\n",
    "\n",
    "try:\n",
    "    from metrics_calculator import calculate_quarterly_metrics\n",
    "    print(\"✅ metrics_calculator.py imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error importing metrics_calculator: {e}\")\n",
    "\n",
    "try:\n",
    "    from trend_analyzer import perform_trend_analysis\n",
    "    print(\"✅ trend_analyzer.py imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error importing trend_analyzer: {e}\")\n",
    "\n",
    "try:\n",
    "    from pipegen_analyzer import process_arr_change_history\n",
    "    print(\"✅ pipegen_analyzer.py imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error importing pipegen_analyzer: {e}\")\n",
    "\n",
    "# Test with sample data\n",
    "try:\n",
    "    import pandas as pd\n",
    "    sample_df = pd.DataFrame({\n",
    "        'Created Date': ['2024-01-15', '2024-02-20'],\n",
    "        'Close Date': ['2024-01-20', '2024-02-25']\n",
    "    })\n",
    "    enhanced_df = enhance_dataframe_dates(sample_df)\n",
    "    print(f\"✅ Date enhancement test successful - added {len(enhanced_df.columns) - 2} new columns\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Date enhancement test failed: {e}\")\n",
    "\n",
    "print(\"\\n🎯 MODULE TEST COMPLETE\")\n",
    "print(\"If all tests passed, the modules work correctly.\")\n",
    "print(\"The issue is likely missing variables from previous cells.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}